{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b360454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:34:02.642436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 13:34:02.828298: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-11 13:34:02.922126: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 13:34:03.560243: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64::/usr/local/cuda-12.3/lib64\n",
      "2024-03-11 13:34:03.560301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64::/usr/local/cuda-12.3/lib64\n",
      "2024-03-11 13:34:03.560305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349122d-b607-4ca4-94be-f0ae766f6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "# 20231225 應該是當初安裝時程式有異，所以keras沒抓到tensorflow預設的版本\n",
    "# 20240104 tf2.10 / tf2.12 version can run\n",
    "# 20240205 tf2.15 version can not run\n",
    "tf_version = tf.__version__\n",
    "\n",
    "pos = tf_version.index('.')\n",
    "pos = tf_version.index('.', pos+1)\n",
    "tf_version = tf_version[0:pos]\n",
    "#print(tf_version)\n",
    "\n",
    "if float(tf_version) < 2.14 :\n",
    "    print(keras.__version__) #-- import keras 可用；tensorflow.keras 沒有這個函式\n",
    "else :\n",
    "    print(\"no method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42a0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:34:08.190377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 13:34:08.236812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.347528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.347985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.388716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.389085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.389153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.389210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 5776 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-03-11 13:34:08.390634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.390716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3-11 13:34:08.390768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.390835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.390887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.390927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 5776 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-03-11 13:34:08.391097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.391158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:08.391207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Test tensorflow-gpu method 1\n",
    "if tf.test.gpu_device_name() :\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else :\n",
    "    print(\"Please install GPUversion of TF\")\n",
    "\n",
    "#tf.test.is_gpu_available() # 該函式在本版本已被棄用\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3555d7-f165-4268-a304-38077bdddb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample=  The cat sat on the mat.\n",
      "word=  The\n",
      "word=  cat\n",
      "word=  sat\n",
      "word=  on\n",
      "word=  the\n",
      "word=  mat.\n",
      "sample=  The dog ate my homework.\n",
      "word=  The\n",
      "word=  dog\n",
      "word=  ate\n",
      "word=  my\n",
      "word=  homework.\n",
      "(2, 10, 11)\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example)\n",
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {} # 宣告一個字典變數\n",
    "for sample in samples:\n",
    "    print(\"sample= \", sample)\n",
    "    for word in sample.split():\n",
    "        print(\"word= \", word)\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\"\"\"\n",
    "numpy zeros() :\n",
    "shape：定義傳回陣列的形狀\n",
    "dtype：產生矩陣的資料型，可選參數，預設為numpy.float64\n",
    "order：{'C'，'F'}，可選，預設：'C'，是否在內容中以行（C）或列（F）順序儲存多維資料。\n",
    "\"\"\"\n",
    "results = np.zeros(shape=(len(samples), \n",
    "                          max_length, \n",
    "                          max(token_index.values()) + 1))\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "print(token_index)\n",
    "print(type(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4471f062-802b-449d-8cbe-f5060709be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example) -- continue ...\n",
    "for i, sample in enumerate(samples):\n",
    "#    print(\"i, sample =\", i, sample)\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "#        print(\"j, word =\", j, word)\n",
    "        index = token_index.get(word)\n",
    "#        print(\"index =\", index)\n",
    "        results[i, j, index] = 1.\n",
    "#        print(results[i, j, index])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38a1c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'sat', 'on', 'the', 'mat.']\n",
      "0 The\n",
      "1 cat\n",
      "2 sat\n",
      "3 on\n",
      "4 the\n",
      "5 mat.\n"
     ]
    }
   ],
   "source": [
    "# Test enumerate() usage\n",
    "\"\"\"\n",
    "for i, sample in enumerate(samples):\n",
    "    print(sample)\n",
    "#    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "    print(list(enumerate(sample.split())))\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length] :\n",
    "#        print(\"j, word =\", j, word)\n",
    "        print(word)\n",
    "    print(\"\\n=== next sentence ===\\n\")\n",
    "\"\"\"\n",
    "    \n",
    "str = \"The cat sat on the mat.\"\n",
    "print(list(str.split()))\n",
    "for i, word in enumerate(list(str.split())) :\n",
    "    print(i, word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b8cefd-9ff4-4c2c-8051-6a90ab558b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== init results ==\n",
      "results object type :  (2, 50, 101)\n",
      "== middle line ==\n",
      "== end line ==\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.2 Character-level one-hot encoding (toy example)\n",
    "# one-hot編碼，逐字元\n",
    "import string\n",
    "np.set_printoptions(threshold=100000)\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# string.printable\n",
    "# value : 「0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$ ...」\n",
    "# 由被視為可打印符號的ASCII字符組成的字符串。\n",
    "# 這是digits, ascii_letters, punctuation 和 whitespace 的總和。\n",
    "characters = string.printable\n",
    "#print(\"characters orders : \", characters)\n",
    "\n",
    "# 20240205 Notes\n",
    "# dict d = {key1 : value1, key2 : value2, ...} \n",
    "# zip(X, Y) => [(x1, y1), (x2, y2), ...]\n",
    "# 20240121 這一行應該寫錯, ref dlwp_ch06_test.ipynb\n",
    "#          output : character= T index=  None i=  0 ,j=  0\n",
    "#token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\"\"\"\n",
    "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', \n",
    "11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', \n",
    "21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', \n",
    "31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', \n",
    "41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', \n",
    "51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', \n",
    "61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', \n",
    "71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', \n",
    "81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', \n",
    "91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'}\n",
    "\"\"\"\n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\"\"\"\n",
    "output :\n",
    "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': \n",
    "11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': \n",
    "21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': \n",
    "31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': \n",
    "41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': \n",
    "51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': \n",
    "61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': \n",
    "71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': \n",
    "81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': \n",
    "91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
    "\"\"\"\n",
    "\n",
    "#print(token_index)\n",
    "#print(token_index.get(5))\n",
    "\n",
    "max_length = 50\n",
    "print(\"== init results ==\")\n",
    "results = np.zeros((len(samples), max_length, len(token_index) + 1)) # 初始化陣列值均為 0\n",
    "#print(results)\n",
    "print(\"results object type : \", results.shape)\n",
    "\n",
    "print(\"== middle line ==\")\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "#    print(\"First loop : i, sample =\", i, sample) # output --> First loop : i, sample = 0 The cat sat on the mat.\n",
    "    for j, character in enumerate(sample):\n",
    "#        print(\"Second Loop : j, character =\", j, character)\n",
    "        \"\"\"\n",
    "        output : \n",
    "          Second Loop : j, character = 0 T\n",
    "          Second Loop : j, character = 1 h\n",
    "          ...\n",
    "        \"\"\"\n",
    "        index = token_index.get(character)\n",
    "#        print(\"character=\", character, \", index= \", index, \", i= \", i, \", j= \", j)\n",
    "        results[i, j, index] = 1. # index == None, 整個陣列的值會被填入 1\n",
    "\n",
    "print(\"== end line ==\")\n",
    "#print(results.shape)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d6155c-16ce-4077-9ec5-e942d84d716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Test None\n",
    "np.set_printoptions(threshold=100000)\n",
    "\n",
    "arr = np.zeros(shape=(2,3,5))\n",
    "print(type(arr))\n",
    "i = 1\n",
    "j = 1\n",
    "k = None\n",
    "#k = 1\n",
    "arr[i, j, k] = 1.\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824454d7-d825-4bfb-8754-92c910c9d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'The cat sat on the mat.'), (1, 'The dog ate my homework.')]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "#token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "print(list(enumerate(samples)))\n",
    "\n",
    "#print(token_index)\n",
    "#print(results.shape)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0002f005-8a66-431f-8b90-2065914eebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttps://blog.droidtown.co/post/188695881747/articutnlp03\\n在語言學的領域裡，語言從最小的「音素」>「音節」>「詞素」>「詞彙」>「詞組」>「句子」>「句組」>「篇章」都是有嚴格的操作定義的。\\n但在 NLP 裡，因為電腦並不知道什麼是音素，更別說理解什麼是詞彙、句子…一類的定義，它只知道「把某些符號擺在一起，然後存入一個記憶體位置。」\\n這樣的操作而已。因此，我們使用 “token” 這個字眼，來表示「某些對人類而言有意義的符號順序」。\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.3 Using Keras for word-level one-hot encoding\n",
    "# 自然語言處理的領域，tokenization 一般會翻譯做分詞，而 tokenizer 一般會翻譯成分詞器。\n",
    "\"\"\"\n",
    "https://ithelp.ithome.com.tw/articles/10291737?sc=rss.iron\n",
    "要教會機器人語言首先我們要寫一本辭典，也就是建立詞彙庫，同樣使用TensorFlow提供的keras中的功能，\n",
    "我們可輸入一些句子，然後根據句子中的單字創造詞彙庫\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "https://github.com/keras-team/keras/blob/v2.15.0/keras/preprocessing/text.py#L329-L343\n",
    "tokenizer 說明 : \n",
    "  http://codewithzhangyi.com/2019/04/23/keras-tokenizer/ \n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\"\"\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index # 找回單詞索引\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)\n",
    "\n",
    "#print(tokenizer)\n",
    "\n",
    "# 轉換成機器人語\n",
    "print(sequences)\n",
    "\n",
    "\"\"\"\n",
    "https://blog.droidtown.co/post/188695881747/articutnlp03\n",
    "在語言學的領域裡，語言從最小的「音素」>「音節」>「詞素」>「詞彙」>「詞組」>「句子」>「句組」>「篇章」都是有嚴格的操作定義的。\n",
    "但在 NLP 裡，因為電腦並不知道什麼是音素，更別說理解什麼是詞彙、句子…一類的定義，它只知道「把某些符號擺在一起，然後存入一個記憶體位置。」\n",
    "這樣的操作而已。因此，我們使用 “token” 這個字眼，來表示「某些對人類而言有意義的符號順序」。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ceb06b-9ef9-4e4b-9570-36a3d6d09373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_str = ['O', 'n', 'l', 'y', ' ', 't', 'h', 'o', 's', 'e', ' ', 'w', 'h', 'o', ' ', 'w', 'i', 'l', 'l', ' ', 'r', 'i', 's', 'k', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'o', ' ', 'f', 'a', 'r', ' ', 'c', 'a', 'n', ' ', 'p', 'o', 's', 's', 'i', 'b', 'l', 'y', ' ', 'f', 'i', 'n', 'd', ' ', 'o', 'u', 't', ' ', 'h', 'o', 'w', ' ', 'f', 'a', 'r', ' ', 'o', 'n', 'e', ' ', 'c', 'a', 'n', ' ', 'g', 'o', '.']\n",
      "[' ', '.', 'O', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y']\n",
      "token2idx = {' ': 0, '.': 1, 'O': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'k': 12, 'l': 13, 'n': 14, 'o': 15, 'p': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'w': 21, 'y': 22}\n",
      "input_ids = [2, 14, 13, 22, 0, 19, 10, 15, 18, 7, 0, 21, 10, 15, 0, 21, 11, 13, 13, 0, 17, 11, 18, 12, 0, 9, 15, 11, 14, 9, 0, 19, 15, 15, 0, 8, 3, 17, 0, 5, 3, 14, 0, 16, 15, 18, 18, 11, 4, 13, 22, 0, 8, 11, 14, 6, 0, 15, 20, 19, 0, 10, 15, 21, 0, 8, 3, 17, 0, 15, 14, 7, 0, 5, 3, 14, 0, 9, 15, 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 入門 ( https://ithelp.ithome.com.tw/articles/10298516 )\n",
    "\n",
    "# Character tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = list(string)\n",
    "print(\"tokenized_str =\", tokenized_str)\n",
    "\n",
    "print(sorted(set(tokenized_str)))\n",
    "# numericalization\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
    "print(\"token2idx =\", token2idx)\n",
    "\n",
    "# 把原始的句子，根據上面這個 set，轉換為數字\n",
    "input_ids = [token2idx[token] for token in tokenized_str]\n",
    "print(\"input_ids =\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a19f0cb-3a92-4a45-bbc0-eb9231d7c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_str=  ['Only', 'those', 'who', 'will', 'risk', 'going', 'too', 'far', 'can', 'possibly', 'find', 'out', 'how', 'far', 'one', 'can', 'go.']\n",
      "token_word2idx=  {'Only': 0, 'can': 1, 'far': 2, 'find': 3, 'go.': 4, 'going': 5, 'how': 6, 'one': 7, 'out': 8, 'possibly': 9, 'risk': 10, 'those': 11, 'too': 12, 'who': 13, 'will': 14}\n",
      "input_ids=  [0, 11, 13, 14, 10, 5, 12, 2, 1, 9, 3, 8, 6, 2, 7, 1, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nenumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在 for 循環當中。\\nsyntax : enumerate(sequence, [start=0])\\n         sequence : 一個序列、迭代器或其他支持迭代對象。\\n         start    : 下標起始位置的值。\\n\\n[set Memo]\\nPython set 集合初始化元素使用 {} 來包住元素，也可以帶入 set() 建構子，但若要建立空的 set 要使用 set()，\\n使用 s = {} 是會建立空 dict，不要搞錯囉！\\nPython 官方文件寫明 set 物件是無序，即使你印出來時發現是按照順序的，所以在使用 set 時請記得不保證有序的，\\n另外 set 裡是不會包含重複的元素。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = string.split()\n",
    "print(\"tokenized_str= \", tokenized_str)\n",
    "\n",
    "# numericalization\n",
    "token_word2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))} # 字典\n",
    "print(\"token_word2idx= \", token_word2idx)\n",
    "\n",
    "# mapping 回 set\n",
    "input_ids = [token_word2idx[token] for token in tokenized_str]\n",
    "print(\"input_ids= \", input_ids)\n",
    "\n",
    "\"\"\"\n",
    "enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在 for 循環當中。\n",
    "syntax : enumerate(sequence, [start=0])\n",
    "         sequence : 一個序列、迭代器或其他支持迭代對象。\n",
    "         start    : 下標起始位置的值。\n",
    "\n",
    "[set Memo]\n",
    "Python set 集合初始化元素使用 {} 來包住元素，也可以帶入 set() 建構子，但若要建立空的 set 要使用 set()，\n",
    "使用 s = {} 是會建立空 dict，不要搞錯囉！\n",
    "Python 官方文件寫明 set 物件是無序，即使你印出來時發現是按照順序的，所以在使用 set 時請記得不保證有序的，\n",
    "另外 set 裡是不會包含重複的元素。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67464683-449c-4542-9eb3-ec510906ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'The cat sat on the mat.'), (1, 'The dog ate my homework.')]\n",
      "[(0, 'The'), (1, 'dog'), (2, 'ate'), (3, 'my'), (4, 'homework.')]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.4 Word-level one-hot encoding with hashing trick (toy example)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "print(list(enumerate(samples)))\n",
    "print(list(enumerate(sample.split())))\n",
    "\n",
    "# 使用hash function計算位置\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1.\n",
    "\n",
    "#print(results.shape)\n",
    "print(results[0:1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e99a4231-618c-4d5e-8dc9-f73f28866c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.embedding.Embedding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[Embedding Memo]\\nhttps://blog.csdn.net/songyunli1111/article/details/85100616\\nEmbedding字面理解是\"嵌入\"，實質是一種映射，從語義空間到向量空間的映射，\\n同時儘可能在向量空間保持原樣本在語義空間的關係，如語義接近的兩個詞彙在向\\n量空間中的位置也比較接近。\\n\\nhttps://qiankunli.github.io/2022/03/02/embedding.html\\nEmbedding 的过程，就是把数据集合映射到向量空间，进而把数据进行向量化的过程。\\nEmbedding 的目标，就是找到一组合适的向量，来刻画现有的数据集合。\\n\\n[word embedding]\\nhttps://blog.csdn.net/qq_41562704/article/details/102662272\\n深度學習模只能處理數值型張量，因此需要將文本轉換為數值張量，即文本向量化。\\n將文本分解成標記token(單詞、字符或n-gram)，將標該與向量關聯的方法\\n常用的one-hot編碼和詞嵌入(word embedding)。\\n\\n現在詞嵌入，每個維度表示一定含義，語義相似的詞的嵌入就相近。\\n最開始時，詞嵌入是每個詞語有固定的詞嵌入，但對一詞多譯的情況並不合理。\\n目前基本都是每個token一個嵌入。\\n\\n詞嵌入的作用是將人類語言映射到幾何空間，利用詞向量之間的幾何關係表示這些詞間的語義關係。\\n\\n可以將一個embedding層理解為字典，接受整數做為輸入，返回相關聯的向量。(單詞索引-->對應的詞向量)\\n在訓練過程中，embedding的權重最開始是隨機的。訓練過程中，利用反向傳播逐漸調節這些詞向量。\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.5 Instantiating an Embedding layer\n",
    "# ref: https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "#      https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# input_dim = 1000 ; outpu_dim = 64\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "print(type(embedding_layer))\n",
    "\n",
    "# 20240210 How to get embedding's shape ??\n",
    "# print(embedding_layer.shape())\n",
    "\n",
    "\"\"\"\n",
    "[Embedding Memo]\n",
    "https://blog.csdn.net/songyunli1111/article/details/85100616\n",
    "Embedding字面理解是\"嵌入\"，實質是一種映射，從語義空間到向量空間的映射，\n",
    "同時儘可能在向量空間保持原樣本在語義空間的關係，如語義接近的兩個詞彙在向\n",
    "量空間中的位置也比較接近。\n",
    "\n",
    "https://qiankunli.github.io/2022/03/02/embedding.html\n",
    "Embedding 的过程，就是把数据集合映射到向量空间，进而把数据进行向量化的过程。\n",
    "Embedding 的目标，就是找到一组合适的向量，来刻画现有的数据集合。\n",
    "\n",
    "[word embedding]\n",
    "https://blog.csdn.net/qq_41562704/article/details/102662272\n",
    "深度學習模只能處理數值型張量，因此需要將文本轉換為數值張量，即文本向量化。\n",
    "將文本分解成標記token(單詞、字符或n-gram)，將標該與向量關聯的方法\n",
    "常用的one-hot編碼和詞嵌入(word embedding)。\n",
    "\n",
    "現在詞嵌入，每個維度表示一定含義，語義相似的詞的嵌入就相近。\n",
    "最開始時，詞嵌入是每個詞語有固定的詞嵌入，但對一詞多譯的情況並不合理。\n",
    "目前基本都是每個token一個嵌入。\n",
    "\n",
    "詞嵌入的作用是將人類語言映射到幾何空間，利用詞向量之間的幾何關係表示這些詞間的語義關係。\n",
    "\n",
    "可以將一個embedding層理解為字典，接受整數做為輸入，返回相關聯的向量。(單詞索引-->對應的詞向量)\n",
    "在訓練過程中，embedding的權重最開始是隨機的。訓練過程中，利用反向傳播逐漸調節這些詞向量。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18082742-c263-4e7a-9f71-106e6541b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.6 Loading the IMDB data for use with an Embedding layer\n",
    "from keras.datasets import imdb\n",
    "# 20240115 新版已將該函數移至它處\n",
    "#from keras import preprocessing\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#print(x_train[:2]) # contents to integer\n",
    "#print(x_test[:2]) # contents to integer\n",
    "#print(y_train[:2]) # comment , only 0 or 1\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "#x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425246de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_1 :  [[0 0 0 0 0 0 0 2 3 4]]\n",
      "list_2 :  [[ 0  0  0  0  0  1  2  3  4  5]\n",
      " [ 0  0  0  0  0  0 11 21 33 44]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nref : https://www.twblogs.net/a/5c113708bd9eee5e40bb23af\\nkeras.preprocessing.sequence.pad_sequences(sequences, \\n    maxlen=None,\\n    dtype='int32',\\n    padding='pre',\\n    truncating='pre', \\n    value=0.)\\nsequences ：浮點數或整數構成的兩層嵌套列表\\nmaxlen    ：None或整數，爲序列的最大長度。大於此長度的序列將被截短，小於此長度的序列將在後部填0.\\ndtype     ：返回的numpy array的數據類型\\npadding   ：‘pre’或‘post’，確定當需要補0時，在序列的起始還是結尾補`\\ntruncating：‘pre’或‘post’，確定當需要截斷序列時，從起始還是結尾截斷\\nvalue     ：浮點數，此值將在填充時代替默認的填充值0\\n返回的是個2維張量，長度爲maxlen\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad_sequences usage \n",
    "list_1 = [[2,3,4]]\n",
    "print(\"list_1 : \", keras.preprocessing.sequence.pad_sequences(list_1, maxlen=10))\n",
    "# array([[0, 0, 0, 0, 0, 0, 0, 2, 3, 4]], dtype=int32)\n",
    "\n",
    "list_2 = [[1,2,3,4,5],[11,21,33,44]]\n",
    "print(\"list_2 : \", keras.preprocessing.sequence.pad_sequences(list_2, maxlen=10))\n",
    "# array([[0, 0, 0, 0, 0, 1, 2, 3, 4, 5]], dtype=int32)\n",
    "\n",
    "\"\"\"\n",
    "ref : https://www.twblogs.net/a/5c113708bd9eee5e40bb23af\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, \n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre', \n",
    "    value=0.)\n",
    "sequences ：浮點數或整數構成的兩層嵌套列表\n",
    "maxlen    ：None或整數，爲序列的最大長度。大於此長度的序列將被截短，小於此長度的序列將在後部填0.\n",
    "dtype     ：返回的numpy array的數據類型\n",
    "padding   ：‘pre’或‘post’，確定當需要補0時，在序列的起始還是結尾補`\n",
    "truncating：‘pre’或‘post’，確定當需要截斷序列時，從起始還是結尾截斷\n",
    "value     ：浮點數，此值將在填充時代替默認的填充值0\n",
    "返回的是個2維張量，長度爲maxlen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9825f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "    15   16 5345   19  178   32]\n",
      " [  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n",
      "    89   78  285   16  145   95]\n",
      " [1352   13  191   79  638   89    2   14    9    8  106  607  624   35\n",
      "   534    6  227    7  129  113]\n",
      " [   7 2804    5    4  559  154  888    7  726   50   26   49 7008   15\n",
      "   566   30  579   21   64 2574]\n",
      " [  15  595   13  784   25 3171   18  165  170  143   19   14    5 7224\n",
      "     6  226  251    7   61  113]\n",
      " [  10   10 1361  173    4  749    2   16 3804    8    4  226   65   12\n",
      "    43  127   24    2   10   10]\n",
      " [  99   76   23    2    7  419  665   40   91   85  108    7    4 2084\n",
      "     5 4773   81   55   52 1901]\n",
      " [ 277 1730   37   25   92  202    6 8848   44   25   28    6   22   15\n",
      "   122   24 4171   72   33   32]\n",
      " [  12  639   21   13   80  140    5  135   15   14    9   31    7    4\n",
      "   118 3672   13   28  126  110]\n",
      " [  78  807    9  375    8 1167    8  794   76    7    4   58    5    4\n",
      "   816    9  243    7   43   50]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.6 after pad_sequences(), 只取前20個字\n",
    "print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623064b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:34:29.379900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:29.380357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:29.380608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:29.380944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:29.381190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 13:34:29.381381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5776 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/625 [=======>......................] - ETA: 0s - loss: 0.6913 - acc: 0.5373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 13:34:29.860227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.6707 - acc: 0.6205 - val_loss: 0.6231 - val_acc: 0.6926\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 899us/step - loss: 0.5449 - acc: 0.7542 - val_loss: 0.5288 - val_acc: 0.7252\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 880us/step - loss: 0.4625 - acc: 0.7880 - val_loss: 0.5035 - val_acc: 0.7416\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 868us/step - loss: 0.4210 - acc: 0.8120 - val_loss: 0.4975 - val_acc: 0.7506\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 882us/step - loss: 0.3921 - acc: 0.8270 - val_loss: 0.4954 - val_acc: 0.7532\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 899us/step - loss: 0.3678 - acc: 0.8407 - val_loss: 0.5011 - val_acc: 0.7558\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 885us/step - loss: 0.3458 - acc: 0.8547 - val_loss: 0.5042 - val_acc: 0.7554\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 928us/step - loss: 0.3263 - acc: 0.8640 - val_loss: 0.5106 - val_acc: 0.7540\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 890us/step - loss: 0.3071 - acc: 0.8730 - val_loss: 0.5198 - val_acc: 0.7518\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 863us/step - loss: 0.2895 - acc: 0.8826 - val_loss: 0.5285 - val_acc: 0.7518\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.7 Using an Embedding layer and classifier on the IMDB data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential() # 全連接層\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "#emb = Embedding(10000, 8, input_length=maxlen)\n",
    "#print(type(emb))\n",
    "#model.add(emb)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20bc55df-4a05-445a-86dc-9c8ab4278d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" (2024.02.12 Wait 2 Finish)\\nREF: https://keras-cn.readthedocs.io/en/latest/layers/embedding_layer/\\n<<< Embedding Usage >>>\\nkeras.layers.embeddings.Embedding(input_dim, \\n                                  output_dim, \\n                                  embeddings_initializer='uniform', \\n                                  embeddings_regularizer=None, \\n                                  activity_regularizer=None, \\n                                  embeddings_constraint=None, \\n                                  mask_zero=False, \\n                                  input_length=None)\\nParameter\\n    input_dim：大或等於0的整數，字典長度，即輸入數據最大下標+1\\n    output_dim：大於0的整數，代表全連接嵌入的維度\\n    embeddings_initializer: 嵌入矩陣的初始化方法，為預定義初始化方法名的字符串，或用於初始化權重的初始化器。\\n                            參考initializers\\n    embeddings_regularizer: 嵌入矩陣的正則項，為Regularizer對象\\n    embeddings_constraint: 嵌入矩陣的約束項，為Constraints對象\\n    mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。\\n    input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\\nInput Parameter\\n    形如 (samples，sequence_length) 的2D张量\\nOutput Parameter\\n    形如 (samples, sequence_length, output_dim) 的3D张量\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" (2024.02.12 Wait 2 Finish)\n",
    "REF: https://keras-cn.readthedocs.io/en/latest/layers/embedding_layer/\n",
    "<<< Embedding Usage >>>\n",
    "keras.layers.embeddings.Embedding(input_dim, \n",
    "                                  output_dim, \n",
    "                                  embeddings_initializer='uniform', \n",
    "                                  embeddings_regularizer=None, \n",
    "                                  activity_regularizer=None, \n",
    "                                  embeddings_constraint=None, \n",
    "                                  mask_zero=False, \n",
    "                                  input_length=None)\n",
    "Parameter\n",
    "    input_dim：大或等於0的整數，字典長度，即輸入數據最大下標+1\n",
    "    output_dim：大於0的整數，代表全連接嵌入的維度\n",
    "    embeddings_initializer: 嵌入矩陣的初始化方法，為預定義初始化方法名的字符串，或用於初始化權重的初始化器。\n",
    "                            參考initializers\n",
    "    embeddings_regularizer: 嵌入矩陣的正則項，為Regularizer對象\n",
    "    embeddings_constraint: 嵌入矩陣的約束項，為Constraints對象\n",
    "    mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。\n",
    "    input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\n",
    "Input Parameter\n",
    "    形如 (samples，sequence_length) 的2D张量\n",
    "Output Parameter\n",
    "    形如 (samples, sequence_length, output_dim) 的3D张量\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4dbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.8 Processing the labels of the raw IMDB data\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\" :\n",
    "    imdb_dir = 'C:\\\\Workspaces\\\\Datasets\\\\aclImdb' \n",
    "elif platform.system() == \"Linux\" :\n",
    "    imdb_dir = '/home/earvin/workspaces/datasets/aclImdb'\n",
    "else :\n",
    "    # Mac path (Not finished)\n",
    "    imdb_dir = '/home/earvin/workspaces/datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            # on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "#            f = open(os.path.join(dir_name, fname))\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                 labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6cc2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A trooper is on the side of the road making sure every1 is obeying the speed limit (doing his job); he then pulls over a woman who appears she is a mother (there is a child in the back seat); he then is telling her what is wrong and BAM...they get killed. Okay, this is the start of what i personally thought would have been a good movie. When I was watching this movie in the theatre I was with some friends. This was our first night out after the summer so we wanted to go and see a good movie. We all decided to see a suspense/thriller that looked good to everyone in the group...this was one of the biggest mistakes of my life. Not only did I waste $7.oo on a movie ticket, but I had to sit through torture for the brain. This movie started off with mystery and suspense and I seriously thought \"this cant be bad\"...I was so wrong. The whole problem with this movie is that it makes no sense; even if you can get passed the bad acting, the \"not so scary\" storyline, and the over all horrible mess this movie was, you will still be puzzled. It\\'s not because you\\'re not smart enough to understand it, it\\'s because no human with a brain could comprehend what this stupid movie is about. Right now you may be thinking \"Oh man! I have to watch this movie just to see if it\\'s as bad as this person says it is\". GET THAT THOUGHT OUT OF YOUR HEAD RIGHT NOW!!! I\\'m trying to save you the trouble of watching this movie by telling you that it is so bad that there is no point in even considering seeing it. Please people don\\'t make the same mistake i did thinking that this movie has potential...it doesn\\'t. I give this movie 1 out of 10 (if I could give a zero I would), and I do not recommend anyone to ever see this movie, you\\'ll be saving yourself many sleepless nights trying to think w.t.f. that freaking movie is about.', 'This movie is a cringe-fest of bad acting and poor set design as well as tacky lines and a lame plot. But it is so much fun to watch. Everything about it is hilarious.The basic plot is a group of scientists from the future travel back in time to capture their evil co-worker who is intent on destroying them all. They catch up with him in the year 1146. The \\'futuristic\\' lab of the scientists from the year 2033 is an eighties-style room with a bunch of \\'futuristic\\' flashing buttons and a time capsule that looks like a lawn shed. The actors deliver their lines with unenthusiastic aplomb, which isn\\'t hard to understand considering that the lines are usually earth-shakers like \" I double-checked everything twice!\" He double checked everything twice? He checked it four times? Not only that, but they feed you the entire premise of the movie in the first five minutes, and continue at a rapid fire pace until they hit the medieval part. When Roger Corman ran out of money. And had to stop travelling through time and consequently different sets. The medieval set is a comic mish-mash of anything from the late 10th century to the 16th century. Any costume they could find, they used. I guess chain mail wasn\\'t on the budget, \\'cause the guys all wear sequined shirts masquerading as armor. The fight scenes are laughable, with men casually throwing themselves onto cardboard swords with abandon and dying in death throws with nary a blow cast.It sounds truly awful, but I enjoy it every time I watch it. The lines alone are enough to have you in fits and everything else pulls together to create a fabulous B-movie that, if you are a connoisseur of corny flicks, I would suggest you see. And once you have, read the review on Unknown Movies. I love hearing them point out all the funny, truly awful bits in the movie.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#print(labels)\n",
    "print(texts[:2])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3788351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.9 Tokenizing the text of the raw IMDB data\n",
    "from keras.preprocessing.text import Tokenizer # 分詞器；tf v2.15 keras\n",
    "# 20240130 tf2.12 \n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100               # 100個單詞後截斷評論\n",
    "training_samples = 200     # 在200個樣本上訓練\n",
    "validation_samples = 10000 # 在10000個樣本上驗證\n",
    "max_words = 10000          # 只考慮數據集中前10000個最常見的單詞\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# 將數據劃分為訓練集和驗證集，首先要打亂數據\n",
    "# 因為一開始數據中的樣本是排序好的\n",
    "# (所有負面評論在前面，然後是所有正面評論)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db38092d-f54d-4944-852d-0d74d3c5a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 13, 4, 3, 55, 56, 6, 57, 58, 2, 59, 31, 60, 18, 32, 18, 61, 19, 2, 3, 62, 33, 20, 7, 4, 34, 63, 64, 5, 35, 14, 65, 7, 4, 66, 1, 67, 33, 4, 3, 68, 6, 36, 15, 1, 69, 70, 71, 8, 21, 5, 72, 25, 73, 74, 75, 76, 4, 77, 26, 78, 37, 27, 16, 79, 80, 9, 81, 8, 1, 38, 82, 1, 39, 83, 6, 1, 36, 15, 1, 38, 84, 4, 85, 86, 87, 88, 9, 3, 89, 6, 39, 90, 91, 2, 3, 21, 92, 22, 93, 40, 3, 94, 95, 1, 96, 97, 25, 19, 9, 98, 99, 100, 101, 102, 5, 103, 104, 22, 1, 19, 17, 105, 106, 107, 40, 10, 41, 28, 14, 42, 43, 41, 28, 14, 42, 43, 28, 7, 108, 109, 44, 45, 22, 20, 16, 110, 11, 1, 111, 112, 6, 1, 13, 8, 1, 113, 114, 115, 2, 116, 117, 3, 118, 119, 120, 121, 16, 122, 1, 46, 123, 124, 125, 126, 127, 47, 6, 128, 2, 129, 5, 130, 131, 132, 21, 2, 133, 134, 135, 1, 46, 31, 4, 3, 136, 137, 138, 6, 139, 15, 1, 140, 141, 48, 5, 1, 142, 48, 143, 144, 16, 145, 146, 16, 147, 10, 148, 149, 150, 151, 26, 1, 152, 153, 1, 154, 27, 155, 156, 157, 158, 18, 159, 1, 160, 161, 17, 162, 9, 163, 164, 165, 166, 167, 168, 169, 9, 170, 2, 171, 8, 172, 173, 9, 174, 3, 175, 176, 7, 177, 49, 50, 20, 10, 178, 7, 179, 21, 10, 35, 7, 1, 19, 180, 17, 181, 5, 51, 11, 8, 182, 2, 14, 52, 183, 184, 5, 185, 3, 186, 187, 13, 22, 29, 11, 17, 3, 188, 6, 189, 190, 10, 191, 192, 11, 193, 2, 194, 11, 51, 195, 1, 196, 26, 197, 198, 10, 199, 200, 37, 201, 47, 27, 1, 202, 49, 50, 203, 8, 1, 13], [204, 24, 13, 53, 15, 205, 4, 23, 206, 24, 207, 4, 2, 208, 209, 210, 211, 212, 45, 5, 213, 214, 215, 12, 12, 23, 30, 216, 217, 218, 219, 220, 221, 23, 30, 222, 223, 2, 23, 224, 225, 1, 226, 17, 12, 12, 34, 227, 228, 229, 11, 29, 230, 231, 54, 232, 2, 233, 234, 15, 235, 236, 237, 20, 29, 238, 239, 54, 25, 240, 241, 242, 243, 244, 245, 246, 30, 247, 18, 32, 7, 4, 44, 248, 249, 53, 250, 12, 12, 251, 252, 7, 253, 254, 1, 255, 256, 2, 257, 5, 14, 52, 258, 8, 259]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer Usage\n",
    "\"\"\"\n",
    "REF: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "※ Deprecated: \n",
    "   `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors and is not \n",
    "   recommended for new code. \n",
    "   Prefer tf.keras.layers.TextVectorization which provides equivalent functionality \n",
    "   through a layer which accepts tf.Tensor input. See the text loading tutorial for \n",
    "   an overview of the layer and text handling in tensorflow.\n",
    "    \n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words) # 分詞器\n",
    "tokenizer.fit_on_texts(texts) # 分詞器方法：實現分詞\n",
    "sequences = tokenizer.texts_to_sequences(texts) # 分詞器方法：輸出向量序列\n",
    "data = pad_sequences(sequences, maxlen=maxlen) # 進行padding (（講話、文章中的）鋪張詞藻，冗詞贅句)\n",
    "\n",
    "\n",
    "\n",
    "https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/1_tokenizer.html\n",
    "Tokenizer 是 NLP pipeline 的核心组件之一。\n",
    "Tokenizer 的目标是：将文本转换为模型可以处理的数据。\n",
    "模型只能处理数字，因此 Tokenizer 需要将文本输入转换为数字输入。\n",
    "通常而言有三种类型的 Tokenizer ：Word-based Tokenizer、Character-based Tokenizer、Subword Tokenizer 。\n",
    "※ Subword Tokenizer說明：https://ithelp.ithome.com.tw/m/articles/10298638\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "#dir(tokenizer)  # 顯示物件屬性\n",
    "vars(tokenizer) # 返回object對象的__dict__屬性，其中object對象可以是模塊、類、實例\n",
    "                 # 或任何其它有__dict__屬性的對象。\n",
    "#tokenizer.__dict__\n",
    "#help(tokenizer) # 說明檔\n",
    "#type(tokenizer)\n",
    "#hasattr(tokenizer, \"num_words\")\n",
    "#callable(tokenizer) # <-- 回呼函數是做啥用 ?? 2024.02.13\n",
    "#print(texts[:3])\n",
    "#print(texts[1:3])\n",
    "#len(texts[:1])\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(texts[1:3])\n",
    "#print(\"=== After call fit_on_texts() ===\")\n",
    "#print(vars(tokenizer))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts[1:3]) # return list object\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b785bb9d-1554-4d67-a287-7a987579ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 13, 4, 3, 55, 56, 6, 57, 58, 2, 59, 31, 60, 18, 32, 18, 61, 19, 2, 3, 62, 33, 20, 7, 4, 34, 63, 64, 5, 35, 14, 65, 7, 4, 66, 1, 67, 33, 4, 3, 68, 6, 36, 15, 1, 69, 70, 71, 8, 21, 5, 72, 25, 73, 74, 75, 76, 4, 77, 26, 78, 37, 27, 16, 79, 80, 9, 81, 8, 1, 38, 82, 1, 39, 83, 6, 1, 36, 15, 1, 38, 84, 4, 85, 86, 87, 88, 9, 3, 89, 6, 39, 90, 91, 2, 3, 21, 92, 22, 93, 40, 3, 94, 95, 1, 96, 97, 25, 19, 9, 98, 99, 100, 101, 102, 5, 103, 104, 22, 1, 19, 17, 105, 106, 107, 40, 10, 41, 28, 14, 42, 43, 41, 28, 14, 42, 43, 28, 7, 108, 109, 44, 45, 22, 20, 16, 110, 11, 1, 111, 112, 6, 1, 13, 8, 1, 113, 114, 115, 2, 116, 117, 3, 118, 119, 120, 121, 16, 122, 1, 46, 123, 124, 125, 126, 127, 47, 6, 128, 2, 129, 5, 130, 131, 132, 21, 2, 133, 134, 135, 1, 46, 31, 4, 3, 136, 137, 138, 6, 139, 15, 1, 140, 141, 48, 5, 1, 142, 48, 143, 144, 16, 145, 146, 16, 147, 10, 148, 149, 150, 151, 26, 1, 152, 153, 1, 154, 27, 155, 156, 157, 158, 18, 159, 1, 160, 161, 17, 162, 9, 163, 164, 165, 166, 167, 168, 169, 9, 170, 2, 171, 8, 172, 173, 9, 174, 3, 175, 176, 7, 177, 49, 50, 20, 10, 178, 7, 179, 21, 10, 35, 7, 1, 19, 180, 17, 181, 5, 51, 11, 8, 182, 2, 14, 52, 183, 184, 5, 185, 3, 186, 187, 13, 22, 29, 11, 17, 3, 188, 6, 189, 190, 10, 191, 192, 11, 193, 2, 194, 11, 51, 195, 1, 196, 26, 197, 198, 10, 199, 200, 37, 201, 47, 27, 1, 202, 49, 50, 203, 8, 1, 13], [204, 24, 13, 53, 15, 205, 4, 23, 206, 24, 207, 4, 2, 208, 209, 210, 211, 212, 45, 5, 213, 214, 215, 12, 12, 23, 30, 216, 217, 218, 219, 220, 221, 23, 30, 222, 223, 2, 23, 224, 225, 1, 226, 17, 12, 12, 34, 227, 228, 229, 11, 29, 230, 231, 54, 232, 2, 233, 234, 15, 235, 236, 237, 20, 29, 238, 239, 54, 25, 240, 241, 242, 243, 244, 245, 246, 30, 247, 18, 32, 7, 4, 44, 248, 249, 53, 250, 12, 12, 251, 252, 7, 253, 254, 1, 255, 256, 2, 257, 5, 14, 52, 258, 8, 259]]\n",
      "[[ 47  27   1 202  49  50 203   8   1  13]\n",
      " [255 256   2 257   5  14  52 258   8 259]]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences usage\n",
    "\"\"\"\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "    padding='pre', truncating='pre', value=0.)\n",
    "sequences：浮点数或整数构成的两层嵌套列表\n",
    "\n",
    "Parameter\n",
    "    maxlen     : None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "    dtype      : 返回的numpy array的数据类型\n",
    "    padding    : ‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "    truncating : ‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "    value      : 浮点数，此值将在填充时代替默认的填充值0\n",
    "\n",
    "Return\n",
    "    返回形如(nb_samples,nb_timesteps)的2D张量\n",
    "\"\"\"\n",
    "print(sequences)\n",
    "data = pad_sequences(sequences, maxlen=10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa82f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nthe:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\\n  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\\n -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\\n -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\\n -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\\n  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\\n  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\\n -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\\n  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\\n  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\\n  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\\n -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\\n -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\\n -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\\n  0.8278    0.27062 ]\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.10 Parsing the GloVe word-embeddings file\n",
    "\n",
    "if platform.system() == \"Windows\" :\n",
    "    glove_dir = 'C:\\\\Workspaces\\\\Datasets\\\\glove.6B'\n",
    "elif platform.system() == \"Linux\" :\n",
    "    glove_dir = '/home/earvin/workspaces/datasets/glove.6B'\n",
    "else :\n",
    "    # Mac path (Not finished)\n",
    "    glove_dir = '/home/earvin/workspaces/datasets/glove.6B'\n",
    "    \n",
    "embeddings_index = {}\n",
    "\n",
    "# on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # the\n",
    "    coefs = np.asarray(values[1:], dtype='float32') # 後面100個預訓練的值\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\"\"\"\n",
    "the:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
    "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
    " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
    " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
    " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
    "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
    "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
    " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
    "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
    "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
    "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
    " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
    " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
    " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
    "  0.8278    0.27062 ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee75f92c-e0e5-4ce4-8c86-0d1324fe3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      ",:[-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ]\n",
      ".:[-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ]\n",
      "of:[-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "to:[-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "=== END ===\n"
     ]
    }
   ],
   "source": [
    "# 顯示 embeddings_index 的值\n",
    "i = 0\n",
    "for key, value in embeddings_index.items() :\n",
    "    if i < 5 :\n",
    "        print('{key}:{value}'.format(key = key, value = value))\n",
    "        i = i + 1\n",
    "    else :\n",
    "        break\n",
    "print(\"=== END ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eff797f-7d5c-4848-9a62-1253d5ccdfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  the\n",
      ", value:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "word:  and\n",
      ", value:  [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "word:  a\n",
      ", value:  [-0.27086    0.044006  -0.02026   -0.17395    0.6444     0.71213\n",
      "  0.3551     0.47138   -0.29637    0.54427   -0.72294   -0.0047612\n",
      "  0.040611   0.043236   0.29729    0.10725    0.40156   -0.53662\n",
      "  0.033382   0.067396   0.64556   -0.085523   0.14103    0.094539\n",
      "  0.74947   -0.194     -0.68739   -0.41741   -0.22807    0.12\n",
      " -0.48999    0.80945    0.045138  -0.11898    0.20161    0.39276\n",
      " -0.20121    0.31354    0.75304    0.25907   -0.11566   -0.029319\n",
      "  0.93499   -0.36067    0.5242     0.23706    0.52715    0.22869\n",
      " -0.51958   -0.79349   -0.20368   -0.50187    0.18748    0.94282\n",
      " -0.44834   -3.6792     0.044183  -0.26751    2.1997     0.241\n",
      " -0.033425   0.69553   -0.64472   -0.0072277  0.89575    0.20015\n",
      "  0.46493    0.61933   -0.1066     0.08691   -0.4623     0.18262\n",
      " -0.15849    0.020791   0.19373    0.063426  -0.31673   -0.48177\n",
      " -1.3848     0.13669    0.96859    0.049965  -0.2738    -0.035686\n",
      " -1.0577    -0.24467    0.90366   -0.12442    0.080776  -0.83401\n",
      "  0.57201    0.088945  -0.42532   -0.018253  -0.079995  -0.28581\n",
      " -0.01089   -0.4923     0.63687    0.23642  ]\n",
      "word:  of\n",
      ", value:  [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "word:  to\n",
      ", value:  [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "word:  is\n",
      ", value:  [-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "word:  br\n",
      ", value:  [ 0.19788    0.25265   -0.28308   -0.11095   -0.73353   -0.4752\n",
      "  0.1335    -0.14369   -0.48716    0.051429   0.60089    0.48182\n",
      "  0.12914    0.69537    0.38632   -0.33128   -0.14934   -0.71853\n",
      "  0.75676    0.18268   -0.14747    0.33359   -0.11076    0.20818\n",
      "  0.061527  -0.02967    0.48226   -0.065332  -0.56994   -0.093896\n",
      "  0.71171    0.48354   -0.29899    0.39467   -0.49209    1.0834\n",
      "  0.10595   -0.1287     0.42798    0.69216    0.60071   -0.6042\n",
      " -0.72736    0.41709    0.41476    0.12809    0.20798   -0.015487\n",
      "  0.32878    0.41328   -0.73539    0.6674    -0.28018    0.1635\n",
      " -0.81071    0.29052   -0.21744    0.81527   -0.067178  -0.38745\n",
      " -0.032753  -0.2202    -0.49692    0.58273    1.1865    -0.252\n",
      " -0.63715    0.45583    0.037555   0.93813   -0.45263   -0.31663\n",
      " -0.52996   -0.45433    0.081431  -0.41059    0.087769  -0.036122\n",
      " -0.0095746 -1.1754    -0.39343   -1.0777     0.224      0.45059\n",
      " -0.36804    0.875     -0.62772   -0.9585    -0.31243    1.2482\n",
      "  0.011367  -0.15143   -0.161      0.036857  -0.022843  -0.58186\n",
      "  0.049536  -0.44063   -0.74265   -0.3215   ]\n",
      "word:  in\n",
      ", value:  [ 0.085703 -0.22201   0.16569   0.13373   0.38239   0.35401   0.01287\n",
      "  0.22461  -0.43817   0.50164  -0.35874  -0.34983   0.055156  0.69648\n",
      " -0.17958   0.067926  0.39101   0.16039  -0.26635  -0.21138   0.53698\n",
      "  0.49379   0.9366    0.66902   0.21793  -0.46642   0.22383  -0.36204\n",
      " -0.17656   0.1748   -0.20367   0.13931   0.019832 -0.10413  -0.20244\n",
      "  0.55003  -0.1546    0.98655  -0.26863  -0.2909   -0.32866  -0.34188\n",
      " -0.16943  -0.42001  -0.046727 -0.16327   0.70824  -0.74911  -0.091559\n",
      " -0.96178  -0.19747   0.10282   0.55221   1.3816   -0.65636  -3.2502\n",
      " -0.31556  -1.2055    1.7709    0.4026   -0.79827   1.1597   -0.33042\n",
      "  0.31382   0.77386   0.22595   0.52471  -0.034053  0.32048   0.079948\n",
      "  0.17752  -0.49426  -0.70045  -0.44569   0.17244   0.20278   0.023292\n",
      " -0.20677  -1.0158    0.18325   0.56752   0.31821  -0.65011   0.68277\n",
      " -0.86585  -0.059392 -0.29264  -0.55668  -0.34705  -0.32895   0.40215\n",
      " -0.12746  -0.20228   0.87368  -0.545     0.79205  -0.20695  -0.074273\n",
      "  0.75808  -0.34243 ]\n",
      "word:  it\n",
      ", value:  [-3.0664e-01  1.6821e-01  9.8511e-01 -3.3606e-01 -2.4160e-01  1.6186e-01\n",
      " -5.3496e-02  4.3010e-01  5.7342e-01 -7.1569e-02  3.6101e-01  2.6729e-01\n",
      "  2.7789e-01 -7.2268e-02  1.3838e-01 -2.6714e-01  1.2999e-01  2.2949e-01\n",
      " -1.8311e-01  5.0163e-01  4.4921e-01 -2.0821e-02  4.2642e-01 -6.8762e-02\n",
      "  4.0337e-01  9.5198e-02 -3.1944e-01 -5.4651e-01 -1.3345e-01 -5.6511e-01\n",
      " -2.0975e-01  1.1592e+00 -1.9400e-01  1.9828e-01 -1.1924e-01  4.1781e-01\n",
      "  6.8383e-03 -2.0537e-01 -5.3375e-01 -5.2225e-01 -3.8227e-01 -6.5833e-03\n",
      "  1.4265e-01 -4.2502e-01 -3.1150e-01  2.7352e-03  7.5093e-01 -4.8218e-01\n",
      " -1.8595e-01 -7.7104e-01 -4.6406e-02 -6.9140e-02  4.1688e-01  1.3235e+00\n",
      " -8.1742e-01 -3.3998e+00 -1.1307e-01 -3.4123e-01  2.0775e+00  6.1369e-01\n",
      "  1.4792e-01  9.3753e-01 -1.0138e-01  2.8426e-01  9.7899e-01 -3.2335e-01\n",
      "  6.3697e-01  5.8308e-01  2.2820e-01 -3.1696e-01  2.1061e-01 -6.5060e-01\n",
      "  2.1653e-01 -2.4347e-01  5.5519e-01 -3.4351e-01 -9.5093e-02 -1.4715e-01\n",
      " -1.2876e+00  3.9310e-01  3.0163e-01 -2.1767e-01 -1.1146e+00  5.1349e-01\n",
      " -1.3410e+00 -3.0381e-01  3.2499e-01 -4.5236e-01 -1.7743e-01 -4.8504e-02\n",
      " -1.2178e-01 -4.2108e-01 -4.0327e-01  3.8452e-02 -3.6084e-01  3.7738e-02\n",
      " -2.1885e-01 -3.8775e-01  3.6916e-01  5.4521e-01]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.11 Preparing the GloVe word-embeddings matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim)) # matrix(10000,100) initialize 0\n",
    "#print(embeddings_index)\n",
    "for word, i in word_index.items():\n",
    "#    if i < 10 :\n",
    "#        print(word, i)\n",
    "#        print(\"word: \", word)\n",
    "#        print(\", value: \", embeddings_index.get(word))\n",
    "        \n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            if i < 10 :\n",
    "                print(\"word: \", word)\n",
    "                print(\", value: \", embeddings_index.get(word))\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "097e832c-d115-45e3-b0f5-57d65b5ad0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n"
     ]
    }
   ],
   "source": [
    "# Display embedding_matrix\n",
    "print(len(embedding_matrix))\n",
    "print(embedding_matrix[1])\n",
    "# Display max_words\n",
    "#print(max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1feb7eec-afde-44e4-9790-682a5ce87b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.12 Model definition\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c714b539-f4ec-458a-b787-22f3114ac281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.13 Loading pretrained word embeddings into the Embedding layer\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fff5042f-3aa5-41a2-b222-96928b609eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[2].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e7cf435-df77-4ff4-b3ed-dc989b610a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 44ms/step - loss: 1.2199 - acc: 0.5250 - val_loss: 0.7029 - val_acc: 0.5007\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.7311 - acc: 0.6550 - val_loss: 0.8231 - val_acc: 0.5056\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.5865 - acc: 0.6300 - val_loss: 0.6935 - val_acc: 0.5375\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3559 - acc: 0.8500 - val_loss: 0.7041 - val_acc: 0.5553\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3731 - acc: 0.8250 - val_loss: 0.9710 - val_acc: 0.5063\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.2355 - acc: 0.9150 - val_loss: 0.8663 - val_acc: 0.5205\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0945 - acc: 0.9950 - val_loss: 0.8666 - val_acc: 0.5318\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0761 - acc: 1.0000 - val_loss: 1.0043 - val_acc: 0.5201\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0579 - acc: 1.0000 - val_loss: 1.3834 - val_acc: 0.5097\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4193 - acc: 0.8600 - val_loss: 0.8937 - val_acc: 0.5394\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.14 Training and evaluation\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad6c8b8a-1812-4c1f-8c5e-69bfbd1b957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing 6.15 Plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2bf6cc3-e2a6-4282-9f66-68ec7fdfb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.6959 - acc: 0.5150 - val_loss: 0.6945 - val_acc: 0.5071\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.4938 - acc: 0.9700 - val_loss: 0.6999 - val_acc: 0.5107\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2795 - acc: 0.9900 - val_loss: 0.6966 - val_acc: 0.5256\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1291 - acc: 1.0000 - val_loss: 0.7321 - val_acc: 0.5152\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0616 - acc: 1.0000 - val_loss: 0.7243 - val_acc: 0.5189\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0312 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.5193\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.7286 - val_acc: 0.5198\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.7411 - val_acc: 0.5187\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.7473 - val_acc: 0.5199\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.7635 - val_acc: 0.5194\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.16 Training the same model without pretrained word embeddings\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60432878-20c1-4e2f-ab98-5251e3be54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.17 Tokenizing the data of the test set\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':           \n",
    "            # on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "            # f = open(os.path.join(dir_name, fname))\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")           \n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "        if label_type == 'neg':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8b5fc5b-eee8-4561-b4ff-258996682a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 0s 584us/step - loss: 0.9323 - acc: 0.5046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9323371052742004, 0.5045599937438965]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.18 Evaluating the model on the test set\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbb5d913-4a05-437d-8f19-3b6707cd68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "#=== Start §6.2 理解循環神經網絡 ===\n",
    "#=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88957d95-957b-4b65-aba0-3085c1869e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Listing 6.19 Pseudocode RNN\u001b[39;00m\n\u001b[1;32m      2\u001b[0m state_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_t \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_sequence\u001b[49m:\n\u001b[1;32m      4\u001b[0m     output_t \u001b[38;5;241m=\u001b[39m f(input_t, state_t)\n\u001b[1;32m      5\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m output_t\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# -- CAN NOT RUN --\n",
    "# Listing 6.19 Pseudocode RNN\n",
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = f(input_t, state_t)\n",
    "    state_t = output_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d64138b-6ee2-4920-9ab3-0cebe75a7320",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -- CAN NOT RUN --\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Listing 6.20 More detailed pseudocode for the RNN\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_t \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_sequence\u001b[49m:\n\u001b[1;32m      4\u001b[0m     output_t \u001b[38;5;241m=\u001b[39m activation(dot(W, input_t) \u001b[38;5;241m+\u001b[39m dot(U, state_t) \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      5\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m output_t\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# -- CAN NOT RUN --\n",
    "# Listing 6.20 More detailed pseudocode for the RNN\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17ffa89c-8b06-436c-afa4-fed0d5bd87f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntanh fun.\\n優點：\\n1. 收斂速度比sigmod函數快\\n2. 輸出是0均值\\n缺點：\\n1. 函數導數輸出為0~1，多次神經網絡計算後，可能趨向於0，產生梯度消失，無法繼續更新參數\\n2. 幂運算複雜，訓練時間長\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.21 Numpy implementation of a simple RNN\n",
    "import numpy as np\n",
    "\n",
    "#timesteps = 100\n",
    "timesteps = 10\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features,)) # 只有一個元素的tuple需加上逗號\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,)) \n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) \n",
    "#    print(output_t)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)\n",
    "\n",
    "\"\"\"\n",
    "tanh fun.\n",
    "優點：\n",
    "1. 收斂速度比sigmod函數快\n",
    "2. 輸出是0均值\n",
    "缺點：\n",
    "1. 函數導數輸出為0~1，多次神經網絡計算後，可能趨向於0，產生梯度消失，無法繼續更新參數\n",
    "2. 幂運算複雜，訓練時間長\n",
    "\n",
    "\n",
    "print(ndarray.ndim)\n",
    "print(ndarray.shape)\n",
    "print(ndarray.dtype)\n",
    "print(ndarray.size)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38e35184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# TEST Listing 6.21\n",
    "\n",
    "#print(type(successive_outputs))\n",
    "#print(len(successive_outputs))\n",
    "#print(successive_outputs)\n",
    "\n",
    "#input_t = np.random.random((timesteps, input_features))\n",
    "#print(input_t)\n",
    "\n",
    "state_t1 = np.zeros((output_features,))\n",
    "print(input_t.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da410d29-6bf8-4038-9e2b-4c824aa23f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Verify\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3797feb-bc6d-4eb8-bf27-3f94fdc4ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb1e93-a158-46fd-a706-f0b1cdb54af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1023dd-e3b6-4507-8019-f2ea8b90c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.22 Preparing the IMDB data\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba88165-0a2d-4b2b-b157-833d743501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.23 Training the model with Embedding and SimpleRNN layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e56cf6-dc49-4bb6-b7c2-173619064651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.24 Plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26e81-5c6f-45fa-9a2a-e971ee1b6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.25 Pseudocode details of the LSTM architecture (1/2)\n",
    "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
    "\n",
    "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\n",
    "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\n",
    "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770b86f-ee3a-4c71-9113-6d2a5c291854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.26 Pseudocode details of the LSTM architecture (2/2)\n",
    "c_t+1 = i_t * k_t + c_t * f_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd170d3-ddba-43d4-9c06-7ac17dcfadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.27 Using the LSTM layer in Keras\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db93dc-580c-4a73-81de-ebb8ff0ee0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.28 Inspecting the data of the Jena weather dataset\n",
    "import os\n",
    "\n",
    "data_dir = '/users/fchollet/Downloads/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44d60f-19c5-47aa-9273-cced970d6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.29 Parsing the data\n",
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf02c4a-97ec-4d43-875f-2b0cda5cc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.30 Plotting the temperature timeseries\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1] ### <1> temperature (in degrees Celsius)\n",
    "plt.plot(range(len(temp)), temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b89d9d-4920-4955-881c-43fcedbfd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.31 Plotting the first 10 days of the temperature timeseries\n",
    "plt.plot(range(1440), temp[:1440])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fbcc5-afa9-4022-bd93-72373167be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.32 Normalizing the data\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efa429-e43a-4646-b5d2-d8793ec58f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.33 Generator yielding timeseries samples and their targets\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index + lookback\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "targets = np.zeros((len(rows),))\n",
    "for j, row in enumerate(rows):\n",
    "    indices = range(rows[j] - lookback, rows[j], step)\n",
    "    samples[j] = data[indices]\n",
    "    targets[j] = data[rows[j] + delay][1]\n",
    "yield samples, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee53c9b-46c5-4412-960d-76ded6b28f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.34 Preparing the training, validation, and test generators\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data, \n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "\n",
    "test_steps = (len(float_data) - 300001 - lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2a0ce-82d6-4455-9c3a-13a74aaf4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.35 Computing the common-sense baseline MAE\n",
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    \n",
    "evaluate_naive_method()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c780e38-bdd8-4162-bcb6-197d5cd68aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.36 Converting the MAE back to a Celsius error\n",
    "celsius_mae = 0.29 * std[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1c9d2-7a9b-4777-b212-4ec84ed07416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.37 Training and evaluating a densely connected model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9108b-c90d-421a-ac39-3021ea5db00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.38 Plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67342-de76-4163-9ce3-6ee1e0dfd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.39 Training and evaluating a GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152c1cd-201a-403c-b63e-a532252706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.40 Training and evaluating a dropout-regularized GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7798eca-9cda-4665-8afc-0f841f5ca531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.41 Training and evaluating a dropout-regularized, stacked GRU model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',dropout=0.1,recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ae483-b297-4354-bed7-c2e395aee2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.42 Training and evaluating an LSTM using reversed sequences\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2f007-9d2a-41c8-b1ba-5500f304b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.43 Training and evaluating a bidirectional LSTM\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,batch_size=128,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646090ff-25bc-4fa9-bfa8-3c6df0157d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.44 Training a bidirectional GRU\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfa3de-bc89-4969-9374-db5ff3facbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.45 Preparing the IMDB data\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969e72a-e6a5-4066-9f72-4531d036fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.46 Training and evaluating a simple 1D convnet on the IMDB data\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,batch_size=128,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c6104-e129-4a87-a825-de87fc8cadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.47 Training and evaluating a simple 1D convnet on the Jena data\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555ec0a-092e-42e7-9434-1be53e527f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.48 Preparing higher-resolution data generators for the Jena dataset\n",
    "step = 3\n",
    "lookback = 720\n",
    "delay = 144\n",
    "\n",
    "train_gen = generator(float_data,lookback=lookback,delay=delay,min_index=0,max_index=200000,shuffle=True,step=step)\n",
    "val_gen = generator(float_data,lookback=lookback,delay=delay,min_index=200001,max_index=300000,step=step)\n",
    "test_gen = generator(float_data,lookback=lookback,delay=delay,min_index=300001,max_index=None,step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f0c71-270f-4a2a-8ac7-bb3a0f18c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.49 Model combining a 1D convolutional base and a GRU layer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
