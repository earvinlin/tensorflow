{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b360454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 14:36:49.527808: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-03 14:36:49.672497: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 14:36:49.672576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 14:36:49.673276: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 14:36:49.749298: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-03 14:36:50.872570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349122d-b607-4ca4-94be-f0ae766f6a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n",
      "no method\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "# 20231225 應該是當初安裝時程式有異，所以keras沒抓到tensorflow預設的版本\n",
    "# 20240104 tf2.10 / tf2.12 version can run\n",
    "# 20240205 tf2.15 version can not run\n",
    "tf_version = tf.__version__\n",
    "\n",
    "pos = tf_version.index('.')\n",
    "pos = tf_version.index('.', pos+1)\n",
    "tf_version = tf_version[0:pos]\n",
    "#print(tf_version)\n",
    "\n",
    "if float(tf_version) < 2.14 :\n",
    "    print(keras.__version__) #-- import keras 可用；tensorflow.keras 沒有這個函式\n",
    "else :\n",
    "    print(\"no method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f42a0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPUversion of TF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tensorflow-gpu method 1\n",
    "if tf.test.gpu_device_name() :\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else :\n",
    "    print(\"Please install GPUversion of TF\")\n",
    "\n",
    "#tf.test.is_gpu_available() # 該函式在本版本已被棄用\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3555d7-f165-4268-a304-38077bdddb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample=  The cat sat on the mat.\n",
      "word=  The\n",
      "word=  cat\n",
      "word=  sat\n",
      "word=  on\n",
      "word=  the\n",
      "word=  mat.\n",
      "sample=  The dog ate my homework.\n",
      "word=  The\n",
      "word=  dog\n",
      "word=  ate\n",
      "word=  my\n",
      "word=  homework.\n",
      "(2, 10, 11)\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "{'The': 1, 'cat': 2, 'sat': 3, 'on': 4, 'the': 5, 'mat.': 6, 'dog': 7, 'ate': 8, 'my': 9, 'homework.': 10}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example)\n",
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {} # 宣告一個字典變數\n",
    "for sample in samples:\n",
    "    print(\"sample= \", sample)\n",
    "    for word in sample.split():\n",
    "        print(\"word= \", word)\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\"\"\"\n",
    "numpy zeros() :\n",
    "shape：定義傳回陣列的形狀\n",
    "dtype：產生矩陣的資料型，可選參數，預設為numpy.float64\n",
    "order：{'C'，'F'}，可選，預設：'C'，是否在內容中以行（C）或列（F）順序儲存多維資料。\n",
    "\"\"\"\n",
    "results = np.zeros(shape=(len(samples), \n",
    "                          max_length, \n",
    "                          max(token_index.values()) + 1))\n",
    "\n",
    "print(results.shape)\n",
    "print(results)\n",
    "print(token_index)\n",
    "print(type(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4471f062-802b-449d-8cbe-f5060709be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Word-level one-hot encoding (toy example) -- continue ...\n",
    "for i, sample in enumerate(samples):\n",
    "#    print(\"i, sample =\", i, sample)\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "#        print(\"j, word =\", j, word)\n",
    "        index = token_index.get(word)\n",
    "#        print(\"index =\", index)\n",
    "        results[i, j, index] = 1.\n",
    "#        print(results[i, j, index])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38a1c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'sat', 'on', 'the', 'mat.']\n",
      "0 The\n",
      "1 cat\n",
      "2 sat\n",
      "3 on\n",
      "4 the\n",
      "5 mat.\n"
     ]
    }
   ],
   "source": [
    "# Test enumerate() usage\n",
    "\"\"\"\n",
    "for i, sample in enumerate(samples):\n",
    "    print(sample)\n",
    "#    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "    print(list(enumerate(sample.split())))\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length] :\n",
    "#        print(\"j, word =\", j, word)\n",
    "        print(word)\n",
    "    print(\"\\n=== next sentence ===\\n\")\n",
    "\"\"\"\n",
    "    \n",
    "str = \"The cat sat on the mat.\"\n",
    "print(list(str.split()))\n",
    "for i, word in enumerate(list(str.split())) :\n",
    "    print(i, word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b8cefd-9ff4-4c2c-8051-6a90ab558b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== init results ==\n",
      "results object type :  (2, 50, 101)\n",
      "== middle line ==\n",
      "== end line ==\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.2 Character-level one-hot encoding (toy example)\n",
    "# one-hot編碼，逐字元\n",
    "import string\n",
    "np.set_printoptions(threshold=100000)\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# string.printable\n",
    "# value : 「0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$ ...」\n",
    "# 由被視為可打印符號的ASCII字符組成的字符串。\n",
    "# 這是digits, ascii_letters, punctuation 和 whitespace 的總和。\n",
    "characters = string.printable\n",
    "#print(\"characters orders : \", characters)\n",
    "\n",
    "# 20240205 Notes\n",
    "# dict d = {key1 : value1, key2 : value2, ...} \n",
    "# zip(X, Y) => [(x1, y1), (x2, y2), ...]\n",
    "# 20240121 這一行應該寫錯, ref dlwp_ch06_test.ipynb\n",
    "#          output : character= T index=  None i=  0 ,j=  0\n",
    "#token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\"\"\"\n",
    "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', \n",
    "11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', \n",
    "21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', \n",
    "31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', \n",
    "41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', \n",
    "51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', \n",
    "61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', \n",
    "71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', \n",
    "81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', \n",
    "91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'}\n",
    "\"\"\"\n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\"\"\"\n",
    "output :\n",
    "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': \n",
    "11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': \n",
    "21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': \n",
    "31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': \n",
    "41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': \n",
    "51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': \n",
    "61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': \n",
    "71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': \n",
    "81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': \n",
    "91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n",
    "\"\"\"\n",
    "\n",
    "#print(token_index)\n",
    "#print(token_index.get(5))\n",
    "\n",
    "max_length = 50\n",
    "print(\"== init results ==\")\n",
    "results = np.zeros((len(samples), max_length, len(token_index) + 1)) # 初始化陣列值均為 0\n",
    "#print(results)\n",
    "print(\"results object type : \", results.shape)\n",
    "\n",
    "print(\"== middle line ==\")\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "#    print(\"First loop : i, sample =\", i, sample) # output --> First loop : i, sample = 0 The cat sat on the mat.\n",
    "    for j, character in enumerate(sample):\n",
    "#        print(\"Second Loop : j, character =\", j, character)\n",
    "        \"\"\"\n",
    "        output : \n",
    "          Second Loop : j, character = 0 T\n",
    "          Second Loop : j, character = 1 h\n",
    "          ...\n",
    "        \"\"\"\n",
    "        index = token_index.get(character)\n",
    "#        print(\"character=\", character, \", index= \", index, \", i= \", i, \", j= \", j)\n",
    "        results[i, j, index] = 1. # index == None, 整個陣列的值會被填入 1\n",
    "\n",
    "print(\"== end line ==\")\n",
    "#print(results.shape)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d6155c-16ce-4077-9ec5-e942d84d716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Test None\n",
    "np.set_printoptions(threshold=100000)\n",
    "\n",
    "arr = np.zeros(shape=(2,3,5))\n",
    "print(type(arr))\n",
    "i = 1\n",
    "j = 1\n",
    "k = None\n",
    "#k = 1\n",
    "arr[i, j, k] = 1.\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824454d7-d825-4bfb-8754-92c910c9d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'The cat sat on the mat.'), (1, 'The dog ate my homework.')]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "#token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "print(list(enumerate(samples)))\n",
    "\n",
    "#print(token_index)\n",
    "#print(results.shape)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0002f005-8a66-431f-8b90-2065914eebb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhttps://blog.droidtown.co/post/188695881747/articutnlp03\\n在語言學的領域裡，語言從最小的「音素」>「音節」>「詞素」>「詞彙」>「詞組」>「句子」>「句組」>「篇章」都是有嚴格的操作定義的。\\n但在 NLP 裡，因為電腦並不知道什麼是音素，更別說理解什麼是詞彙、句子…一類的定義，它只知道「把某些符號擺在一起，然後存入一個記憶體位置。」\\n這樣的操作而已。因此，我們使用 “token” 這個字眼，來表示「某些對人類而言有意義的符號順序」。\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.3 Using Keras for word-level one-hot encoding\n",
    "# 自然語言處理的領域，tokenization 一般會翻譯做分詞，而 tokenizer 一般會翻譯成分詞器。\n",
    "\"\"\"\n",
    "https://ithelp.ithome.com.tw/articles/10291737?sc=rss.iron\n",
    "要教會機器人語言首先我們要寫一本辭典，也就是建立詞彙庫，同樣使用TensorFlow提供的keras中的功能，\n",
    "我們可輸入一些句子，然後根據句子中的單字創造詞彙庫\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "https://github.com/keras-team/keras/blob/v2.15.0/keras/preprocessing/text.py#L329-L343\n",
    "tokenizer 說明 : \n",
    "  http://codewithzhangyi.com/2019/04/23/keras-tokenizer/ \n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\"\"\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index # 找回單詞索引\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)\n",
    "\n",
    "#print(tokenizer)\n",
    "\n",
    "# 轉換成機器人語\n",
    "print(sequences)\n",
    "\n",
    "\"\"\"\n",
    "https://blog.droidtown.co/post/188695881747/articutnlp03\n",
    "在語言學的領域裡，語言從最小的「音素」>「音節」>「詞素」>「詞彙」>「詞組」>「句子」>「句組」>「篇章」都是有嚴格的操作定義的。\n",
    "但在 NLP 裡，因為電腦並不知道什麼是音素，更別說理解什麼是詞彙、句子…一類的定義，它只知道「把某些符號擺在一起，然後存入一個記憶體位置。」\n",
    "這樣的操作而已。因此，我們使用 “token” 這個字眼，來表示「某些對人類而言有意義的符號順序」。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ceb06b-9ef9-4e4b-9570-36a3d6d09373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_str = ['O', 'n', 'l', 'y', ' ', 't', 'h', 'o', 's', 'e', ' ', 'w', 'h', 'o', ' ', 'w', 'i', 'l', 'l', ' ', 'r', 'i', 's', 'k', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 't', 'o', 'o', ' ', 'f', 'a', 'r', ' ', 'c', 'a', 'n', ' ', 'p', 'o', 's', 's', 'i', 'b', 'l', 'y', ' ', 'f', 'i', 'n', 'd', ' ', 'o', 'u', 't', ' ', 'h', 'o', 'w', ' ', 'f', 'a', 'r', ' ', 'o', 'n', 'e', ' ', 'c', 'a', 'n', ' ', 'g', 'o', '.']\n",
      "[' ', '.', 'O', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y']\n",
      "token2idx = {' ': 0, '.': 1, 'O': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'k': 12, 'l': 13, 'n': 14, 'o': 15, 'p': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'w': 21, 'y': 22}\n",
      "input_ids = [2, 14, 13, 22, 0, 19, 10, 15, 18, 7, 0, 21, 10, 15, 0, 21, 11, 13, 13, 0, 17, 11, 18, 12, 0, 9, 15, 11, 14, 9, 0, 19, 15, 15, 0, 8, 3, 17, 0, 5, 3, 14, 0, 16, 15, 18, 18, 11, 4, 13, 22, 0, 8, 11, 14, 6, 0, 15, 20, 19, 0, 10, 15, 21, 0, 8, 3, 17, 0, 15, 14, 7, 0, 5, 3, 14, 0, 9, 15, 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 入門 ( https://ithelp.ithome.com.tw/articles/10298516 )\n",
    "\n",
    "# Character tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = list(string)\n",
    "print(\"tokenized_str =\", tokenized_str)\n",
    "\n",
    "print(sorted(set(tokenized_str)))\n",
    "# numericalization\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
    "print(\"token2idx =\", token2idx)\n",
    "\n",
    "# 把原始的句子，根據上面這個 set，轉換為數字\n",
    "input_ids = [token2idx[token] for token in tokenized_str]\n",
    "print(\"input_ids =\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a19f0cb-3a92-4a45-bbc0-eb9231d7c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_str=  ['Only', 'those', 'who', 'will', 'risk', 'going', 'too', 'far', 'can', 'possibly', 'find', 'out', 'how', 'far', 'one', 'can', 'go.']\n",
      "token_word2idx=  {'Only': 0, 'can': 1, 'far': 2, 'find': 3, 'go.': 4, 'going': 5, 'how': 6, 'one': 7, 'out': 8, 'possibly': 9, 'risk': 10, 'those': 11, 'too': 12, 'who': 13, 'will': 14}\n",
      "input_ids=  [0, 11, 13, 14, 10, 5, 12, 2, 1, 9, 3, 8, 6, 2, 7, 1, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nenumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在 for 循環當中。\\nsyntax : enumerate(sequence, [start=0])\\n         sequence : 一個序列、迭代器或其他支持迭代對象。\\n         start    : 下標起始位置的值。\\n\\n[set Memo]\\nPython set 集合初始化元素使用 {} 來包住元素，也可以帶入 set() 建構子，但若要建立空的 set 要使用 set()，\\n使用 s = {} 是會建立空 dict，不要搞錯囉！\\nPython 官方文件寫明 set 物件是無序，即使你印出來時發現是按照順序的，所以在使用 set 時請記得不保證有序的，\\n另外 set 裡是不會包含重複的元素。\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "string = \"Only those who will risk going too far can possibly find out how far one can go.\"\n",
    "tokenized_str = string.split()\n",
    "print(\"tokenized_str= \", tokenized_str)\n",
    "\n",
    "# numericalization\n",
    "token_word2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))} # 字典\n",
    "print(\"token_word2idx= \", token_word2idx)\n",
    "\n",
    "# mapping 回 set\n",
    "input_ids = [token_word2idx[token] for token in tokenized_str]\n",
    "print(\"input_ids= \", input_ids)\n",
    "\n",
    "\"\"\"\n",
    "enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在 for 循環當中。\n",
    "syntax : enumerate(sequence, [start=0])\n",
    "         sequence : 一個序列、迭代器或其他支持迭代對象。\n",
    "         start    : 下標起始位置的值。\n",
    "\n",
    "[set Memo]\n",
    "Python set 集合初始化元素使用 {} 來包住元素，也可以帶入 set() 建構子，但若要建立空的 set 要使用 set()，\n",
    "使用 s = {} 是會建立空 dict，不要搞錯囉！\n",
    "Python 官方文件寫明 set 物件是無序，即使你印出來時發現是按照順序的，所以在使用 set 時請記得不保證有序的，\n",
    "另外 set 裡是不會包含重複的元素。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67464683-449c-4542-9eb3-ec510906ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'The cat sat on the mat.'), (1, 'The dog ate my homework.')]\n",
      "[(0, 'The'), (1, 'dog'), (2, 'ate'), (3, 'my'), (4, 'homework.')]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.4 Word-level one-hot encoding with hashing trick (toy example)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "print(list(enumerate(samples)))\n",
    "print(list(enumerate(sample.split())))\n",
    "\n",
    "# 使用hash function計算位置\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1.\n",
    "\n",
    "#print(results.shape)\n",
    "print(results[0:1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e99a4231-618c-4d5e-8dc9-f73f28866c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.layers.core.embedding.Embedding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[Embedding Memo]\\nhttps://blog.csdn.net/songyunli1111/article/details/85100616\\nEmbedding字面理解是\"嵌入\"，實質是一種映射，從語義空間到向量空間的映射，\\n同時儘可能在向量空間保持原樣本在語義空間的關係，如語義接近的兩個詞彙在向\\n量空間中的位置也比較接近。\\n\\nhttps://qiankunli.github.io/2022/03/02/embedding.html\\nEmbedding 的过程，就是把数据集合映射到向量空间，进而把数据进行向量化的过程。\\nEmbedding 的目标，就是找到一组合适的向量，来刻画现有的数据集合。\\n\\n[word embedding]\\nhttps://blog.csdn.net/qq_41562704/article/details/102662272\\n深度學習模只能處理數值型張量，因此需要將文本轉換為數值張量，即文本向量化。\\n將文本分解成標記token(單詞、字符或n-gram)，將標該與向量關聯的方法\\n常用的one-hot編碼和詞嵌入(word embedding)。\\n\\n現在詞嵌入，每個維度表示一定含義，語義相似的詞的嵌入就相近。\\n最開始時，詞嵌入是每個詞語有固定的詞嵌入，但對一詞多譯的情況並不合理。\\n目前基本都是每個token一個嵌入。\\n\\n詞嵌入的作用是將人類語言映射到幾何空間，利用詞向量之間的幾何關係表示這些詞間的語義關係。\\n\\n可以將一個embedding層理解為字典，接受整數做為輸入，返回相關聯的向量。(單詞索引-->對應的詞向量)\\n在訓練過程中，embedding的權重最開始是隨機的。訓練過程中，利用反向傳播逐漸調節這些詞向量。\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.5 Instantiating an Embedding layer\n",
    "# ref: https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "#      https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "print(type(embedding_layer))\n",
    "\n",
    "# 20240210 How to get embedding's shape ??\n",
    "#print(embedding_layer.shape())\n",
    "\n",
    "\"\"\"\n",
    "[Embedding Memo]\n",
    "https://blog.csdn.net/songyunli1111/article/details/85100616\n",
    "Embedding字面理解是\"嵌入\"，實質是一種映射，從語義空間到向量空間的映射，\n",
    "同時儘可能在向量空間保持原樣本在語義空間的關係，如語義接近的兩個詞彙在向\n",
    "量空間中的位置也比較接近。\n",
    "\n",
    "https://qiankunli.github.io/2022/03/02/embedding.html\n",
    "Embedding 的过程，就是把数据集合映射到向量空间，进而把数据进行向量化的过程。\n",
    "Embedding 的目标，就是找到一组合适的向量，来刻画现有的数据集合。\n",
    "\n",
    "[word embedding]\n",
    "https://blog.csdn.net/qq_41562704/article/details/102662272\n",
    "深度學習模只能處理數值型張量，因此需要將文本轉換為數值張量，即文本向量化。\n",
    "將文本分解成標記token(單詞、字符或n-gram)，將標該與向量關聯的方法\n",
    "常用的one-hot編碼和詞嵌入(word embedding)。\n",
    "\n",
    "現在詞嵌入，每個維度表示一定含義，語義相似的詞的嵌入就相近。\n",
    "最開始時，詞嵌入是每個詞語有固定的詞嵌入，但對一詞多譯的情況並不合理。\n",
    "目前基本都是每個token一個嵌入。\n",
    "\n",
    "詞嵌入的作用是將人類語言映射到幾何空間，利用詞向量之間的幾何關係表示這些詞間的語義關係。\n",
    "\n",
    "可以將一個embedding層理解為字典，接受整數做為輸入，返回相關聯的向量。(單詞索引-->對應的詞向量)\n",
    "在訓練過程中，embedding的權重最開始是隨機的。訓練過程中，利用反向傳播逐漸調節這些詞向量。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18082742-c263-4e7a-9f71-106e6541b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.6 Loading the IMDB data for use with an Embedding layer\n",
    "from keras.datasets import imdb\n",
    "# 20240115 新版已將該函數移至它處\n",
    "#from keras import preprocessing\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "#print(x_train[:2]) # contents to integer\n",
    "#print(y_train[:2]) # comment , only 0 or 1\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "#x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425246de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_1 :  [[0 0 0 0 0 0 0 2 3 4]]\n",
      "list_2 :  [[ 0  0  0  0  0  1  2  3  4  5]\n",
      " [ 0  0  0  0  0  0 11 21 33 44]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nref : https://www.twblogs.net/a/5c113708bd9eee5e40bb23af\\nkeras.preprocessing.sequence.pad_sequences(sequences, \\n    maxlen=None,\\n    dtype='int32',\\n    padding='pre',\\n    truncating='pre', \\n    value=0.)\\nsequences ：浮點數或整數構成的兩層嵌套列表\\nmaxlen    ：None或整數，爲序列的最大長度。大於此長度的序列將被截短，小於此長度的序列將在後部填0.\\ndtype     ：返回的numpy array的數據類型\\npadding   ：‘pre’或‘post’，確定當需要補0時，在序列的起始還是結尾補`\\ntruncating：‘pre’或‘post’，確定當需要截斷序列時，從起始還是結尾截斷\\nvalue     ：浮點數，此值將在填充時代替默認的填充值0\\n返回的是個2維張量，長度爲maxlen\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad_sequences usage \n",
    "list_1 = [[2,3,4]]\n",
    "print(\"list_1 : \", keras.preprocessing.sequence.pad_sequences(list_1, maxlen=10))\n",
    "# array([[0, 0, 0, 0, 0, 0, 0, 2, 3, 4]], dtype=int32)\n",
    "\n",
    "list_2 = [[1,2,3,4,5],[11,21,33,44]]\n",
    "print(\"list_2 : \", keras.preprocessing.sequence.pad_sequences(list_2, maxlen=10))\n",
    "# array([[0, 0, 0, 0, 0, 1, 2, 3, 4, 5]], dtype=int32)\n",
    "\n",
    "\"\"\"\n",
    "ref : https://www.twblogs.net/a/5c113708bd9eee5e40bb23af\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, \n",
    "    maxlen=None,\n",
    "    dtype='int32',\n",
    "    padding='pre',\n",
    "    truncating='pre', \n",
    "    value=0.)\n",
    "sequences ：浮點數或整數構成的兩層嵌套列表\n",
    "maxlen    ：None或整數，爲序列的最大長度。大於此長度的序列將被截短，小於此長度的序列將在後部填0.\n",
    "dtype     ：返回的numpy array的數據類型\n",
    "padding   ：‘pre’或‘post’，確定當需要補0時，在序列的起始還是結尾補`\n",
    "truncating：‘pre’或‘post’，確定當需要截斷序列時，從起始還是結尾截斷\n",
    "value     ：浮點數，此值將在填充時代替默認的填充值0\n",
    "返回的是個2維張量，長度爲maxlen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9825f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "    15   16 5345   19  178   32]\n",
      " [  23    4 1690   15   16    4 1355    5   28    6   52  154  462   33\n",
      "    89   78  285   16  145   95]\n",
      " [1352   13  191   79  638   89    2   14    9    8  106  607  624   35\n",
      "   534    6  227    7  129  113]\n",
      " [   7 2804    5    4  559  154  888    7  726   50   26   49 7008   15\n",
      "   566   30  579   21   64 2574]\n",
      " [  15  595   13  784   25 3171   18  165  170  143   19   14    5 7224\n",
      "     6  226  251    7   61  113]\n",
      " [  10   10 1361  173    4  749    2   16 3804    8    4  226   65   12\n",
      "    43  127   24    2   10   10]\n",
      " [  99   76   23    2    7  419  665   40   91   85  108    7    4 2084\n",
      "     5 4773   81   55   52 1901]\n",
      " [ 277 1730   37   25   92  202    6 8848   44   25   28    6   22   15\n",
      "   122   24 4171   72   33   32]\n",
      " [  12  639   21   13   80  140    5  135   15   14    9   31    7    4\n",
      "   118 3672   13   28  126  110]\n",
      " [  78  807    9  375    8 1167    8  794   76    7    4   58    5    4\n",
      "   816    9  243    7   43   50]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623064b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 4ms/step - loss: 0.6733 - acc: 0.6135 - val_loss: 0.6296 - val_acc: 0.6910\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.5508 - acc: 0.7517 - val_loss: 0.5315 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4645 - acc: 0.7874 - val_loss: 0.5029 - val_acc: 0.7424\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4214 - acc: 0.8102 - val_loss: 0.4954 - val_acc: 0.7514\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3921 - acc: 0.8267 - val_loss: 0.4954 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3680 - acc: 0.8394 - val_loss: 0.4985 - val_acc: 0.7526\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3462 - acc: 0.8518 - val_loss: 0.5060 - val_acc: 0.7542\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3258 - acc: 0.8637 - val_loss: 0.5100 - val_acc: 0.7498\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3069 - acc: 0.8742 - val_loss: 0.5175 - val_acc: 0.7482\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2884 - acc: 0.8842 - val_loss: 0.5246 - val_acc: 0.7490\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.7 Using an Embedding layer and classifier on the IMDB data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential() # 全連接層\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "#emb = Embedding(10000, 8, input_length=maxlen)\n",
    "#print(type(emb))\n",
    "#model.add(emb)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "20bc55df-4a05-445a-86dc-9c8ab4278d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" (2024.02.12 Wait 2 Finish)\\nREF: https://keras-cn.readthedocs.io/en/latest/layers/embedding_layer/\\n<<< Embedding Usage >>>\\nkeras.layers.embeddings.Embedding(input_dim, \\n                                  output_dim, \\n                                  embeddings_initializer='uniform', \\n                                  embeddings_regularizer=None, \\n                                  activity_regularizer=None, \\n                                  embeddings_constraint=None, \\n                                  mask_zero=False, \\n                                  input_length=None)\\nParameter\\n    input_dim：大或等於0的整數，字典長度，即輸入數據最大下標+1\\n    output_dim：大於0的整數，代表全連接嵌入的維度\\n    embeddings_initializer: 嵌入矩陣的初始化方法，為預定義初始化方法名的字符串，或用於初始化權重的初始化器。\\n                            參考initializers\\n    embeddings_regularizer: 嵌入矩陣的正則項，為Regularizer對象\\n    embeddings_constraint: 嵌入矩陣的約束項，為Constraints對象\\n    mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。\\n    input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\\nInput Parameter\\n    形如 (samples，sequence_length) 的2D张量\\nOutput Parameter\\n    形如 (samples, sequence_length, output_dim) 的3D张量\\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" (2024.02.12 Wait 2 Finish)\n",
    "REF: https://keras-cn.readthedocs.io/en/latest/layers/embedding_layer/\n",
    "<<< Embedding Usage >>>\n",
    "keras.layers.embeddings.Embedding(input_dim, \n",
    "                                  output_dim, \n",
    "                                  embeddings_initializer='uniform', \n",
    "                                  embeddings_regularizer=None, \n",
    "                                  activity_regularizer=None, \n",
    "                                  embeddings_constraint=None, \n",
    "                                  mask_zero=False, \n",
    "                                  input_length=None)\n",
    "Parameter\n",
    "    input_dim：大或等於0的整數，字典長度，即輸入數據最大下標+1\n",
    "    output_dim：大於0的整數，代表全連接嵌入的維度\n",
    "    embeddings_initializer: 嵌入矩陣的初始化方法，為預定義初始化方法名的字符串，或用於初始化權重的初始化器。\n",
    "                            參考initializers\n",
    "    embeddings_regularizer: 嵌入矩陣的正則項，為Regularizer對象\n",
    "    embeddings_constraint: 嵌入矩陣的約束項，為Constraints對象\n",
    "    mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。\n",
    "    input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。\n",
    "Input Parameter\n",
    "    形如 (samples，sequence_length) 的2D张量\n",
    "Output Parameter\n",
    "    形如 (samples, sequence_length, output_dim) 的3D张量\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d4dbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.8 Processing the labels of the raw IMDB data\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\" :\n",
    "    imdb_dir = 'C:\\\\Workspaces\\\\Datasets\\\\aclImdb' \n",
    "elif platform.system() == \"Linux\" :\n",
    "    imdb_dir = '/home/earvin/workspaces/datasets/aclImdb'\n",
    "else :\n",
    "    # Mac path (Not finished)\n",
    "    imdb_dir = '/home/earvin/workspaces/datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            # on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "#            f = open(os.path.join(dir_name, fname))\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                 labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6cc2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#print(labels)\n",
    "#print(texts[:2])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3788351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72633 unique tokens.\n",
      "Shape of data tensor: (17243, 100)\n",
      "Shape of label tensor: (17243,)\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.9 Tokenizing the text of the raw IMDB data\n",
    "from keras.preprocessing.text import Tokenizer # 分詞器；tf v2.15 keras\n",
    "# 20240130 tf2.12 \n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100               # 100個單詞後截斷評論\n",
    "training_samples = 200     # 在200個樣本上訓練\n",
    "validation_samples = 10000 # 在10000個樣本上驗證\n",
    "max_words = 10000          # 只考慮數據集中前10000個最常見的單詞\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# 將數據劃分為訓練集和驗證集，首先要打亂數據\n",
    "# 因為一開始數據中的樣本是排序好的\n",
    "# (所有負面評論在前面，然後是所有正面評論)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db38092d-f54d-4944-852d-0d74d3c5a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 3, 76, 77, 7, 41, 1, 78, 7, 42, 43, 14, 1, 24, 79, 2, 80, 81, 44, 45, 46, 47, 82, 5, 83, 84, 7, 1, 85, 86, 87, 88, 7, 8, 48, 2, 8, 49, 89, 50, 90, 91, 92, 93, 25, 9, 14, 94, 7, 15, 2, 15, 95, 96, 97, 10, 6, 16, 98, 17, 51, 99, 100, 101, 26, 27, 51, 102, 103, 11, 8, 48, 26, 104, 105, 1, 106, 107, 108, 26, 109, 11, 8, 49, 2, 17, 1, 52, 110, 8, 111, 112, 113, 114, 115, 116, 6, 18, 117, 12, 1, 118, 2, 119, 120, 121, 28, 1, 53, 29, 19, 30, 122, 20, 6, 18, 2, 41, 1, 53, 123, 124, 125, 126, 1, 54, 24, 13, 21, 55, 127, 128, 18, 129, 130, 10, 29, 19, 56, 57, 131, 1, 54, 24, 132, 13, 2, 12, 133, 20, 55, 134, 13, 135, 31, 58, 59, 136, 137, 4, 4, 138, 6, 3, 139, 140, 42, 141, 32, 14, 142, 33, 43, 22, 17, 2, 60, 34, 44, 143, 22, 17, 6, 16, 27, 144, 145, 31, 3, 146, 5, 147, 148, 23, 56, 33, 15, 2, 5, 33, 149, 1, 35, 150, 25, 58, 6, 12, 36, 46, 47, 30, 5, 151, 152, 61, 1, 153, 1, 154, 30, 155, 156, 31, 157, 158, 22, 159, 160, 11, 1, 161, 18, 162, 23, 37, 163, 164, 165], [166, 7, 36, 62, 167, 168, 169, 63, 64, 170, 65, 60, 9, 171, 5, 172, 173, 174, 175, 176, 36, 9, 177, 3, 178, 2, 179, 50, 180, 2, 15, 66, 181, 182, 7, 67, 4, 4, 1, 38, 183, 184, 185, 186, 187, 188, 22, 189, 28, 3, 190, 61, 191, 192, 193, 12, 1, 194, 32, 3, 37, 5, 195, 196, 13, 20, 6, 38, 68, 11, 1, 39, 40, 64, 3, 197, 1, 35, 16, 10, 198, 5, 69, 4, 4, 29, 19, 2, 199, 200, 14, 201, 202, 203, 2, 204, 13, 3, 205, 70, 71, 5, 206, 70, 1, 67, 62, 207, 66, 57, 208, 21, 1, 209, 4, 4, 1, 38, 3, 210, 10, 211, 212, 2, 213, 214, 215, 72, 216, 217, 218, 10, 219, 220, 221, 32, 3, 222, 71, 73, 223, 2, 59, 3, 5, 224, 225, 226, 34, 227, 11, 73, 16, 74, 228, 68, 21, 19, 2, 1, 39, 40, 2, 74, 229, 28, 9, 230, 4, 4, 1, 35, 231, 232, 72, 25, 233, 65, 3, 1, 39, 40, 2, 1, 234, 23, 235, 75, 236, 237, 37, 238, 239, 69, 240, 63, 4, 4, 241, 242, 21, 6, 243, 2, 27, 244, 245, 246, 2, 247, 248, 12, 249, 9, 250, 251, 34, 252, 253, 5, 254, 7, 255, 2, 45, 23, 52, 75, 20, 1, 256]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer Usage\n",
    "\"\"\"\n",
    "REF: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "※ Deprecated: \n",
    "   `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors and is not \n",
    "   recommended for new code. \n",
    "   Prefer tf.keras.layers.TextVectorization which provides equivalent functionality \n",
    "   through a layer which accepts tf.Tensor input. See the text loading tutorial for \n",
    "   an overview of the layer and text handling in tensorflow.\n",
    "    \n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words) # 分詞器\n",
    "tokenizer.fit_on_texts(texts) # 分詞器方法：實現分詞\n",
    "sequences = tokenizer.texts_to_sequences(texts) # 分詞器方法：輸出向量序列\n",
    "data = pad_sequences(sequences, maxlen=maxlen) # 進行padding (（講話、文章中的）鋪張詞藻，冗詞贅句)\n",
    "\n",
    "\n",
    "\n",
    "https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/1_tokenizer.html\n",
    "Tokenizer 是 NLP pipeline 的核心组件之一。\n",
    "Tokenizer 的目标是：将文本转换为模型可以处理的数据。\n",
    "模型只能处理数字，因此 Tokenizer 需要将文本输入转换为数字输入。\n",
    "通常而言有三种类型的 Tokenizer ：Word-based Tokenizer、Character-based Tokenizer、Subword Tokenizer 。\n",
    "※ Subword Tokenizer說明：https://ithelp.ithome.com.tw/m/articles/10298638\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "#dir(tokenizer)  # 顯示物件屬性\n",
    "vars(tokenizer) # 返回object對象的__dict__屬性，其中object對象可以是模塊、類、實例\n",
    "                 # 或任何其它有__dict__屬性的對象。\n",
    "#tokenizer.__dict__\n",
    "#help(tokenizer) # 說明檔\n",
    "#type(tokenizer)\n",
    "#hasattr(tokenizer, \"num_words\")\n",
    "#callable(tokenizer) # <-- 回呼函數是做啥用 ?? 2024.02.13\n",
    "#print(texts[:3])\n",
    "#print(texts[1:3])\n",
    "#len(texts[:1])\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(texts[1:3])\n",
    "#print(\"=== After call fit_on_texts() ===\")\n",
    "#print(vars(tokenizer))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts[1:3]) # return list object\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b785bb9d-1554-4d67-a287-7a987579ad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 3, 76, 77, 7, 41, 1, 78, 7, 42, 43, 14, 1, 24, 79, 2, 80, 81, 44, 45, 46, 47, 82, 5, 83, 84, 7, 1, 85, 86, 87, 88, 7, 8, 48, 2, 8, 49, 89, 50, 90, 91, 92, 93, 25, 9, 14, 94, 7, 15, 2, 15, 95, 96, 97, 10, 6, 16, 98, 17, 51, 99, 100, 101, 26, 27, 51, 102, 103, 11, 8, 48, 26, 104, 105, 1, 106, 107, 108, 26, 109, 11, 8, 49, 2, 17, 1, 52, 110, 8, 111, 112, 113, 114, 115, 116, 6, 18, 117, 12, 1, 118, 2, 119, 120, 121, 28, 1, 53, 29, 19, 30, 122, 20, 6, 18, 2, 41, 1, 53, 123, 124, 125, 126, 1, 54, 24, 13, 21, 55, 127, 128, 18, 129, 130, 10, 29, 19, 56, 57, 131, 1, 54, 24, 132, 13, 2, 12, 133, 20, 55, 134, 13, 135, 31, 58, 59, 136, 137, 4, 4, 138, 6, 3, 139, 140, 42, 141, 32, 14, 142, 33, 43, 22, 17, 2, 60, 34, 44, 143, 22, 17, 6, 16, 27, 144, 145, 31, 3, 146, 5, 147, 148, 23, 56, 33, 15, 2, 5, 33, 149, 1, 35, 150, 25, 58, 6, 12, 36, 46, 47, 30, 5, 151, 152, 61, 1, 153, 1, 154, 30, 155, 156, 31, 157, 158, 22, 159, 160, 11, 1, 161, 18, 162, 23, 37, 163, 164, 165], [166, 7, 36, 62, 167, 168, 169, 63, 64, 170, 65, 60, 9, 171, 5, 172, 173, 174, 175, 176, 36, 9, 177, 3, 178, 2, 179, 50, 180, 2, 15, 66, 181, 182, 7, 67, 4, 4, 1, 38, 183, 184, 185, 186, 187, 188, 22, 189, 28, 3, 190, 61, 191, 192, 193, 12, 1, 194, 32, 3, 37, 5, 195, 196, 13, 20, 6, 38, 68, 11, 1, 39, 40, 64, 3, 197, 1, 35, 16, 10, 198, 5, 69, 4, 4, 29, 19, 2, 199, 200, 14, 201, 202, 203, 2, 204, 13, 3, 205, 70, 71, 5, 206, 70, 1, 67, 62, 207, 66, 57, 208, 21, 1, 209, 4, 4, 1, 38, 3, 210, 10, 211, 212, 2, 213, 214, 215, 72, 216, 217, 218, 10, 219, 220, 221, 32, 3, 222, 71, 73, 223, 2, 59, 3, 5, 224, 225, 226, 34, 227, 11, 73, 16, 74, 228, 68, 21, 19, 2, 1, 39, 40, 2, 74, 229, 28, 9, 230, 4, 4, 1, 35, 231, 232, 72, 25, 233, 65, 3, 1, 39, 40, 2, 1, 234, 23, 235, 75, 236, 237, 37, 238, 239, 69, 240, 63, 4, 4, 241, 242, 21, 6, 243, 2, 27, 244, 245, 246, 2, 247, 248, 12, 249, 9, 250, 251, 34, 252, 253, 5, 254, 7, 255, 2, 45, 23, 52, 75, 20, 1, 256]]\n",
      "[[ 11   1 161  18 162  23  37 163 164 165]\n",
      " [  7 255   2  45  23  52  75  20   1 256]]\n"
     ]
    }
   ],
   "source": [
    "# pad_sequences usage\n",
    "\"\"\"\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "    padding='pre', truncating='pre', value=0.)\n",
    "sequences：浮点数或整数构成的两层嵌套列表\n",
    "\n",
    "Parameter\n",
    "    maxlen     : None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
    "    dtype      : 返回的numpy array的数据类型\n",
    "    padding    : ‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
    "    truncating : ‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "    value      : 浮点数，此值将在填充时代替默认的填充值0\n",
    "\n",
    "Return\n",
    "    返回形如(nb_samples,nb_timesteps)的2D张量\n",
    "\"\"\"\n",
    "print(sequences)\n",
    "data = pad_sequences(sequences, maxlen=10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa82f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nthe:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\\n  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\\n -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\\n -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\\n -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\\n  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\\n  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\\n -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\\n  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\\n  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\\n  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\\n -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\\n -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\\n -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\\n  0.8278    0.27062 ]\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.10 Parsing the GloVe word-embeddings file\n",
    "\n",
    "if platform.system() == \"Windows\" :\n",
    "    glove_dir = 'C:\\\\Workspaces\\\\Datasets\\\\glove.6B'\n",
    "elif platform.system() == \"Linux\" :\n",
    "    glove_dir = '/home/earvin/workspaces/datasets/glove.6B'\n",
    "else :\n",
    "    # Mac path (Not finished)\n",
    "    glove_dir = '/home/earvin/workspaces/datasets/glove.6B'\n",
    "    \n",
    "embeddings_index = {}\n",
    "\n",
    "# on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # the\n",
    "    coefs = np.asarray(values[1:], dtype='float32') # 後面100個預訓練的值\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\"\"\"\n",
    "the:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
    "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
    " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
    " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
    " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
    "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
    "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
    " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
    "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
    "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
    "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
    " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
    " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
    " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
    "  0.8278    0.27062 ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ee75f92c-e0e5-4ce4-8c86-0d1324fe3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the:[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      ",:[-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ]\n",
      ".:[-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ]\n",
      "of:[-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "to:[-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "=== END ===\n"
     ]
    }
   ],
   "source": [
    "# 顯示 embeddings_index 的值\n",
    "i = 0\n",
    "for key, value in embeddings_index.items() :\n",
    "    if i < 5 :\n",
    "        print('{key}:{value}'.format(key = key, value = value))\n",
    "        i = i + 1\n",
    "    else :\n",
    "        break\n",
    "print(\"=== END ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1eff797f-7d5c-4848-9a62-1253d5ccdfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  the\n",
      ", value:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "word:  a\n",
      ", value:  [-0.27086    0.044006  -0.02026   -0.17395    0.6444     0.71213\n",
      "  0.3551     0.47138   -0.29637    0.54427   -0.72294   -0.0047612\n",
      "  0.040611   0.043236   0.29729    0.10725    0.40156   -0.53662\n",
      "  0.033382   0.067396   0.64556   -0.085523   0.14103    0.094539\n",
      "  0.74947   -0.194     -0.68739   -0.41741   -0.22807    0.12\n",
      " -0.48999    0.80945    0.045138  -0.11898    0.20161    0.39276\n",
      " -0.20121    0.31354    0.75304    0.25907   -0.11566   -0.029319\n",
      "  0.93499   -0.36067    0.5242     0.23706    0.52715    0.22869\n",
      " -0.51958   -0.79349   -0.20368   -0.50187    0.18748    0.94282\n",
      " -0.44834   -3.6792     0.044183  -0.26751    2.1997     0.241\n",
      " -0.033425   0.69553   -0.64472   -0.0072277  0.89575    0.20015\n",
      "  0.46493    0.61933   -0.1066     0.08691   -0.4623     0.18262\n",
      " -0.15849    0.020791   0.19373    0.063426  -0.31673   -0.48177\n",
      " -1.3848     0.13669    0.96859    0.049965  -0.2738    -0.035686\n",
      " -1.0577    -0.24467    0.90366   -0.12442    0.080776  -0.83401\n",
      "  0.57201    0.088945  -0.42532   -0.018253  -0.079995  -0.28581\n",
      " -0.01089   -0.4923     0.63687    0.23642  ]\n",
      "word:  and\n",
      ", value:  [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "word:  of\n",
      ", value:  [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "word:  to\n",
      ", value:  [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "word:  br\n",
      ", value:  [ 0.19788    0.25265   -0.28308   -0.11095   -0.73353   -0.4752\n",
      "  0.1335    -0.14369   -0.48716    0.051429   0.60089    0.48182\n",
      "  0.12914    0.69537    0.38632   -0.33128   -0.14934   -0.71853\n",
      "  0.75676    0.18268   -0.14747    0.33359   -0.11076    0.20818\n",
      "  0.061527  -0.02967    0.48226   -0.065332  -0.56994   -0.093896\n",
      "  0.71171    0.48354   -0.29899    0.39467   -0.49209    1.0834\n",
      "  0.10595   -0.1287     0.42798    0.69216    0.60071   -0.6042\n",
      " -0.72736    0.41709    0.41476    0.12809    0.20798   -0.015487\n",
      "  0.32878    0.41328   -0.73539    0.6674    -0.28018    0.1635\n",
      " -0.81071    0.29052   -0.21744    0.81527   -0.067178  -0.38745\n",
      " -0.032753  -0.2202    -0.49692    0.58273    1.1865    -0.252\n",
      " -0.63715    0.45583    0.037555   0.93813   -0.45263   -0.31663\n",
      " -0.52996   -0.45433    0.081431  -0.41059    0.087769  -0.036122\n",
      " -0.0095746 -1.1754    -0.39343   -1.0777     0.224      0.45059\n",
      " -0.36804    0.875     -0.62772   -0.9585    -0.31243    1.2482\n",
      "  0.011367  -0.15143   -0.161      0.036857  -0.022843  -0.58186\n",
      "  0.049536  -0.44063   -0.74265   -0.3215   ]\n",
      "word:  is\n",
      ", value:  [-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "word:  in\n",
      ", value:  [ 0.085703 -0.22201   0.16569   0.13373   0.38239   0.35401   0.01287\n",
      "  0.22461  -0.43817   0.50164  -0.35874  -0.34983   0.055156  0.69648\n",
      " -0.17958   0.067926  0.39101   0.16039  -0.26635  -0.21138   0.53698\n",
      "  0.49379   0.9366    0.66902   0.21793  -0.46642   0.22383  -0.36204\n",
      " -0.17656   0.1748   -0.20367   0.13931   0.019832 -0.10413  -0.20244\n",
      "  0.55003  -0.1546    0.98655  -0.26863  -0.2909   -0.32866  -0.34188\n",
      " -0.16943  -0.42001  -0.046727 -0.16327   0.70824  -0.74911  -0.091559\n",
      " -0.96178  -0.19747   0.10282   0.55221   1.3816   -0.65636  -3.2502\n",
      " -0.31556  -1.2055    1.7709    0.4026   -0.79827   1.1597   -0.33042\n",
      "  0.31382   0.77386   0.22595   0.52471  -0.034053  0.32048   0.079948\n",
      "  0.17752  -0.49426  -0.70045  -0.44569   0.17244   0.20278   0.023292\n",
      " -0.20677  -1.0158    0.18325   0.56752   0.31821  -0.65011   0.68277\n",
      " -0.86585  -0.059392 -0.29264  -0.55668  -0.34705  -0.32895   0.40215\n",
      " -0.12746  -0.20228   0.87368  -0.545     0.79205  -0.20695  -0.074273\n",
      "  0.75808  -0.34243 ]\n",
      "word:  i\n",
      ", value:  [-0.046539   0.61966    0.56647   -0.46584   -1.189      0.44599\n",
      "  0.066035   0.3191     0.14679   -0.22119    0.79239    0.29905\n",
      "  0.16073    0.025324   0.18678   -0.31001   -0.28108    0.60515\n",
      " -1.0654     0.52476    0.064152   1.0358    -0.40779   -0.38011\n",
      "  0.30801    0.59964   -0.26991   -0.76035    0.94222   -0.46919\n",
      " -0.18278    0.90652    0.79671    0.24825    0.25713    0.6232\n",
      " -0.44768    0.65357    0.76902   -0.51229   -0.44333   -0.21867\n",
      "  0.3837    -1.1483    -0.94398   -0.15062    0.30012   -0.57806\n",
      "  0.20175   -1.6591    -0.079195   0.026423   0.22051    0.99714\n",
      " -0.57539   -2.7266     0.31448    0.70522    1.4381     0.99126\n",
      "  0.13976    1.3474    -1.1753     0.0039503  1.0298     0.064637\n",
      "  0.90887    0.82872   -0.47003   -0.10575    0.5916    -0.4221\n",
      "  0.57331   -0.54114    0.10768    0.39784   -0.048744   0.064596\n",
      " -0.61437   -0.286      0.5067    -0.49758   -0.8157     0.16408\n",
      " -1.963     -0.26693   -0.37593   -0.95847   -0.8584    -0.71577\n",
      " -0.32343   -0.43121    0.41392    0.28374   -0.70931    0.15003\n",
      " -0.2154    -0.37616   -0.032502   0.8062   ]\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.11 Preparing the GloVe word-embeddings matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim)) # matrix(10000,100) initialize 0\n",
    "#print(embeddings_index)\n",
    "for word, i in word_index.items():\n",
    "#    if i < 10 :\n",
    "#        print(word, i)\n",
    "#        print(\"word: \", word)\n",
    "#        print(\", value: \", embeddings_index.get(word))\n",
    "        \n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            if i < 10 :\n",
    "                print(\"word: \", word)\n",
    "                print(\", value: \", embeddings_index.get(word))\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "097e832c-d115-45e3-b0f5-57d65b5ad0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999]\n"
     ]
    }
   ],
   "source": [
    "# Display embedding_matrix\n",
    "print(len(embedding_matrix))\n",
    "print(embedding_matrix[1])\n",
    "# Display max_words\n",
    "#print(max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1feb7eec-afde-44e4-9790-682a5ce87b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1320065 (5.04 MB)\n",
      "Trainable params: 1320065 (5.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.12 Model definition\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c714b539-f4ec-458a-b787-22f3114ac281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.13 Loading pretrained word embeddings into the Embedding layer\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fff5042f-3aa5-41a2-b222-96928b609eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(model.layers[4].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6e7cf435-df77-4ff4-b3ed-dc989b610a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 1.4305 - acc: 0.6100 - val_loss: 0.9202 - val_acc: 0.7211\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.5756 - acc: 0.7350 - val_loss: 0.6901 - val_acc: 0.5308\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.5236 - acc: 0.7800 - val_loss: 1.0584 - val_acc: 0.2963\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.3654 - acc: 0.8100 - val_loss: 0.7768 - val_acc: 0.7210\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.1736 - acc: 0.9600 - val_loss: 0.7276 - val_acc: 0.7203\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.0936 - acc: 0.9950 - val_loss: 1.3900 - val_acc: 0.7210\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.1671 - acc: 0.9250 - val_loss: 0.6542 - val_acc: 0.6462\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.6748 - val_acc: 0.7045\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.8416 - val_acc: 0.7202\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.9688 - val_acc: 0.7212\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.14 Training and evaluation\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ad6c8b8a-1812-4c1f-8c5e-69bfbd1b957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing 6.15 Plotting the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c2bf6cc3-e2a6-4282-9f66-68ec7fdfb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                320032    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1320065 (5.04 MB)\n",
      "Trainable params: 1320065 (5.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 73ms/step - loss: 0.6818 - acc: 0.6150 - val_loss: 0.6507 - val_acc: 0.7212\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 0.5265 - acc: 0.8150 - val_loss: 0.6201 - val_acc: 0.7211\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.3749 - acc: 0.9400 - val_loss: 0.5993 - val_acc: 0.7211\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 0.2388 - acc: 0.9750 - val_loss: 0.5998 - val_acc: 0.7138\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.1317 - acc: 1.0000 - val_loss: 0.5981 - val_acc: 0.7205\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0759 - acc: 1.0000 - val_loss: 0.6069 - val_acc: 0.7218\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.6207 - val_acc: 0.7217\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.6175 - val_acc: 0.7190\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.6345 - val_acc: 0.7199\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.6472 - val_acc: 0.7204\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.16 Training the same model without pretrained word embeddings\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "60432878-20c1-4e2f-ab98-5251e3be54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.17 Tokenizing the data of the test set\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':           \n",
    "            # on windows platform, 會因為編碼問題報錯，所以要加上指定編碼\n",
    "            # f = open(os.path.join(dir_name, fname))\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf-8\")           \n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "        if label_type == 'neg':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "            \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8b5fc5b-eee8-4561-b4ff-258996682a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/539 [==============================] - 1s 1ms/step - loss: 1.0248 - acc: 0.7242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0248031616210938, 0.7242358922958374]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.18 Evaluating the model on the test set\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5d913-4a05-437d-8f19-3b6707cd68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "#=== Start §6.2 理解循環神經網絡 ===\n",
    "#=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "88957d95-957b-4b65-aba0-3085c1869e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Listing 6.19 Pseudocode RNN\u001b[39;00m\n\u001b[0;32m      2\u001b[0m state_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_t \u001b[38;5;129;01min\u001b[39;00m input_sequence:\n\u001b[0;32m      4\u001b[0m     output_t \u001b[38;5;241m=\u001b[39m f(input_t, state_t)\n\u001b[0;32m      5\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m output_t\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# Listing 6.19 Pseudocode RNN\n",
    "state_t = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = f(input_t, state_t)\n",
    "    state_t = output_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64138b-6ee2-4920-9ab3-0cebe75a7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.20 More detailed pseudocode for the RNN\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "    state_t = output_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffa89c-8b06-436c-afa4-fed0d5bd87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.21 Numpy implementation of a simple RNN\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features,))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
    "    successive_outputs.append(output_t)\n",
    "    state_t = output_t\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da410d29-6bf8-4038-9e2b-4c824aa23f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Verify\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3797feb-bc6d-4eb8-bf27-3f94fdc4ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb1e93-a158-46fd-a706-f0b1cdb54af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1023dd-e3b6-4507-8019-f2ea8b90c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.22 Preparing the IMDB data\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba88165-0a2d-4b2b-b157-833d743501c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.23 Training the model with Embedding and SimpleRNN layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e56cf6-dc49-4bb6-b7c2-173619064651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.24 Plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f26e81-5c6f-45fa-9a2a-e971ee1b6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.25 Pseudocode details of the LSTM architecture (1/2)\n",
    "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
    "\n",
    "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\n",
    "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\n",
    "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770b86f-ee3a-4c71-9113-6d2a5c291854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.26 Pseudocode details of the LSTM architecture (2/2)\n",
    "c_t+1 = i_t * k_t + c_t * f_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd170d3-ddba-43d4-9c06-7ac17dcfadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.27 Using the LSTM layer in Keras\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db93dc-580c-4a73-81de-ebb8ff0ee0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.28 Inspecting the data of the Jena weather dataset\n",
    "import os\n",
    "\n",
    "data_dir = '/users/fchollet/Downloads/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44d60f-19c5-47aa-9273-cced970d6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.29 Parsing the data\n",
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf02c4a-97ec-4d43-875f-2b0cda5cc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.30 Plotting the temperature timeseries\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1] ### <1> temperature (in degrees Celsius)\n",
    "plt.plot(range(len(temp)), temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b89d9d-4920-4955-881c-43fcedbfd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.31 Plotting the first 10 days of the temperature timeseries\n",
    "plt.plot(range(1440), temp[:1440])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fbcc5-afa9-4022-bd93-72373167be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.32 Normalizing the data\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efa429-e43a-4646-b5d2-d8793ec58f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.33 Generator yielding timeseries samples and their targets\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index + lookback\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "targets = np.zeros((len(rows),))\n",
    "for j, row in enumerate(rows):\n",
    "    indices = range(rows[j] - lookback, rows[j], step)\n",
    "    samples[j] = data[indices]\n",
    "    targets[j] = data[rows[j] + delay][1]\n",
    "yield samples, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee53c9b-46c5-4412-960d-76ded6b28f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.34 Preparing the training, validation, and test generators\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data, \n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "\n",
    "test_steps = (len(float_data) - 300001 - lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2a0ce-82d6-4455-9c3a-13a74aaf4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.35 Computing the common-sense baseline MAE\n",
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    \n",
    "evaluate_naive_method()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c780e38-bdd8-4162-bcb6-197d5cd68aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.36 Converting the MAE back to a Celsius error\n",
    "celsius_mae = 0.29 * std[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1c9d2-7a9b-4777-b212-4ec84ed07416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.37 Training and evaluating a densely connected model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9108b-c90d-421a-ac39-3021ea5db00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.38 Plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67342-de76-4163-9ce3-6ee1e0dfd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.39 Training and evaluating a GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152c1cd-201a-403c-b63e-a532252706a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.40 Training and evaluating a dropout-regularized GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7798eca-9cda-4665-8afc-0f841f5ca531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.41 Training and evaluating a dropout-regularized, stacked GRU model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',dropout=0.1,recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ae483-b297-4354-bed7-c2e395aee2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.42 Training and evaluating an LSTM using reversed sequences\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2f007-9d2a-41c8-b1ba-5500f304b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.43 Training and evaluating a bidirectional LSTM\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,batch_size=128,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646090ff-25bc-4fa9-bfa8-3c6df0157d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.44 Training a bidirectional GRU\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfa3de-bc89-4969-9374-db5ff3facbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.45 Preparing the IMDB data\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969e72a-e6a5-4066-9f72-4531d036fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.46 Training and evaluating a simple 1D convnet on the IMDB data\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,batch_size=128,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c6104-e129-4a87-a825-de87fc8cadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.47 Training and evaluating a simple 1D convnet on the Jena data\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555ec0a-092e-42e7-9434-1be53e527f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.48 Preparing higher-resolution data generators for the Jena dataset\n",
    "step = 3\n",
    "lookback = 720\n",
    "delay = 144\n",
    "\n",
    "train_gen = generator(float_data,lookback=lookback,delay=delay,min_index=0,max_index=200000,shuffle=True,step=step)\n",
    "val_gen = generator(float_data,lookback=lookback,delay=delay,min_index=200001,max_index=300000,step=step)\n",
    "test_gen = generator(float_data,lookback=lookback,delay=delay,min_index=300001,max_index=None,step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f0c71-270f-4a2a-8ac7-bb3a0f18c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.49 Model combining a 1D convolutional base and a GRU layer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
